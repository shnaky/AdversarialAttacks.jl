var documenterSearchIndex = {"docs":
[{"location":"attack_interface/#Attack-Interface","page":"Attack Interface","title":"Attack Interface","text":"This page documents the shared attack abstractions.","category":"section"},{"location":"attack_interface/#AdversarialAttacks.Attack.AbstractAttack","page":"Attack Interface","title":"AdversarialAttacks.Attack.AbstractAttack","text":"Abstract supertype for all adversarial attacks.\n\nExpected interface (to be implemented per concrete attack):\n\nname(::AbstractAttack)::String\nhyperparameters(::AbstractAttack)::Dict{String,Any}\ncraft(sample, model, atk::AbstractAttack; kwargs...) returning an adversarial example\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.Attack.BlackBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.Attack.BlackBoxAttack","text":"Abstract type for black-box adversarial attacks.\n\nBlack-box attacks only have access to the model's input-output behavior, without knowledge of the model's internals, gradients, or architecture. These attacks typically rely on query-based methods (e.g., optimization via repeated queries) or transferability from surrogate models.\n\nUse BlackBoxAttack when you do not have access to the model's internal parameters or gradients, such as in deployed systems or APIs.  In contrast, use WhiteBoxAttack when you have full access to the model's internals and can leverage gradient information for crafting adversarial examples.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.Attack.WhiteBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.Attack.WhiteBoxAttack","text":"Abstract type for white-box adversarial attacks.\n\nWhite-box attacks have full access to the model's internals, including gradients, weights, and architecture. This enables the use of gradient-based optimization and other techniques to craft adversarial examples.\n\nUse this type when the attacker can inspect and manipulate the model's internal parameters and computations. If only input-output access is available, use BlackBoxAttack instead.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.Attack.craft-Tuple{Any, Any, AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.Attack.craft","text":"craft(sample, model, attack::AbstractAttack; kwargs...) -> adversarial_sample\n\nCraft an adversarial example by applying the attack to a sample.\n\nArguments\n\nsample: Input sample to perturb (e.g., image, text)\nmodel: Target model to attack\nattack::AbstractAttack: Attack configuration and algorithm\nkwargs...: Additional attack-specific parameters\n\nReturns\n\nAdversarial example with the same shape as the input sample\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#AdversarialAttacks.Attack.hyperparameters-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.Attack.hyperparameters","text":"Return hyperparameters for an attack\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#AdversarialAttacks.Attack.name-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.Attack.name","text":"Human-readable name for an attack\n\n\n\n\n\n","category":"method"},{"location":"#AdversarialAttacks","page":"Home","title":"AdversarialAttacks","text":"Documentation for AdversarialAttacks.\n\n","category":"section"}]
}
