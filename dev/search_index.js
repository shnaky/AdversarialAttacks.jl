var documenterSearchIndex = {"docs":
[{"location":"examples/whitebox_fgsm_flux_mnist/#White-Box-–-FGSM-(Flux,-MNIST)","page":"White-Box – FGSM (Flux, MNIST)","title":"White-Box – FGSM (Flux, MNIST)","text":"This tutorial illustrates a white‑box attack using the Fast Gradient Sign Method (FGSM) against a convolutional neural network trained on MNIST handwritten digits. The goal is to show how small, gradient‑based perturbations can flip the model’s prediction while remaining almost imperceptible.","category":"section"},{"location":"examples/whitebox_fgsm_flux_mnist/#What-the-script-does","page":"White-Box – FGSM (Flux, MNIST)","title":"What the script does","text":"Loads a subset of MNIST, normalizes pixel values to 0 1, and trains a simple CNN with Flux.\nWraps the trained a CNN model in Flux and crafts an adversarial example for a correctly classified digit using FGSM with a small L_infty budget.\nReports clean vs adversarial probabilities, the drop in true‑class confidence, and whether the predicted digit changes.","category":"section"},{"location":"examples/whitebox_fgsm_flux_mnist/#Visual-summary","page":"White-Box – FGSM (Flux, MNIST)","title":"Visual summary","text":"The effect of the attack is summarized by the following figure, included in the documentation as:\n\n(Image: FGSM attack on MNIST)\n\nThe three panels show:\n\nOriginal (digit = 0): The clean MNIST image that the model classifies correctly.\nAdversarial (digit = 7): The perturbed image after the FGSM attack, which the model now misclassifies as a different digit.\nPerturbation (epsilon = 00015): A heatmap of the pixel‑wise difference, highlighting that only a thin band of pixels around the digit is modified while the overall shape remains visually similar.\n\nThis figure makes the white‑box threat model and the subtlety of the perturbation immediately apparent.","category":"section"},{"location":"attack_interface/#Attack-Interface","page":"Attack Interface","title":"Attack Interface","text":"This page documents the shared attack abstractions.","category":"section"},{"location":"attack_interface/#AdversarialAttacks.AbstractAttack","page":"Attack Interface","title":"AdversarialAttacks.AbstractAttack","text":"Abstract supertype for all adversarial attacks.\n\nExpected interface (to be implemented per concrete attack):\n\nname(::AbstractAttack)::String\nhyperparameters(::AbstractAttack)::Dict{String,Any}\ncraft(sample, model, atk::AbstractAttack; kwargs...) returning an adversarial example\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.WhiteBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.WhiteBoxAttack","text":"Abstract type for white-box adversarial attacks.\n\nWhite-box attacks have full access to the model's internals, including gradients, weights, and architecture. This enables the use of gradient-based optimization and other techniques to craft adversarial examples.\n\nUse this type when the attacker can inspect and manipulate the model's internal parameters and computations. If only input-output access is available, use BlackBoxAttack instead.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.BlackBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.BlackBoxAttack","text":"Abstract type for black-box adversarial attacks.\n\nBlack-box attacks only have access to the model's input-output behavior, without knowledge of the model's internals, gradients, or architecture. These attacks typically rely on query-based methods (e.g., optimization via repeated queries) or transferability from surrogate models.\n\nUse BlackBoxAttack when you do not have access to the model's internal parameters or gradients, such as in deployed systems or APIs.  In contrast, use WhiteBoxAttack when you have full access to the model's internals and can leverage gradient information for crafting adversarial examples.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.name-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.name","text":"name(atk::AbstractAttack) -> String\n\nHuman-readable name for an attack.\n\nReturns\n\nString: String representation of the attack type.\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#AdversarialAttacks.hyperparameters-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.hyperparameters","text":"hyperparameters(atk::AbstractAttack) -> Dict{String,Any}\n\nReturn hyperparameters for an attack.\n\nReturns\n\nDict{String,Any}: Dictionary of attack hyperparameters.\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#AdversarialAttacks.craft","page":"Attack Interface","title":"AdversarialAttacks.craft","text":"craft(sample, model, attack::AbstractAttack; kwargs...) -> adversarial_sample\n\nCraft an adversarial example by applying the attack to a sample.\n\nArguments\n\nsample: Input sample to perturb (e.g., image, text)\nmodel: Target model to attack\nattack::AbstractAttack: Attack configuration and algorithm\nkwargs...: Additional attack-specific parameters\n\nReturns\n\nAdversarial example with the same shape as the input sample\n\n\n\n\n\ncraft(sample, model::Chain, attack::BasicRandomSearch)\n\nPerform a black-box adversarial attack on the given model using the provided sample using the Basic Random Search variant SimBA.\n\nArguments\n\nsample: The input sample to be changed.\nmodel::Chain: The machine learning (deep learning, classical machine learning) model to be attacked.\nattack::BasicRandomSearch: An instance of the BasicRandomSearch (BlackBox) attack.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\ncraft(sample, model::DecisionTreeClassifier, attack::BasicRandomSearch)\n\nPerform a black-box adversarial attack on a DecisionTreeClassifier using BasicRandomSearch (SimBA).\n\nArguments\n\nsample: NamedTuple with data and label fields.\nmodel::DecisionTreeClassifier: DecisionTree.jl classifier to attack.\nattack::BasicRandomSearch: Attack instance with epsilon and optional bounds.\n\nReturns\n\nAdversarial example (same shape as sample.data).\n\n\n\n\n\ncraft(sample, model, attack::FGSM)\n\nPerform a Fast Gradient Sign Method (FGSM) white-box adversarial attack on the given model using the provided sample.\n\nArguments\n\nsample: Input sample as a named tuple with data and label.\nmodel::FluxModel: The machine learning (deep learning) model to be attacked.\nattack::FGSM: An instance of the FGSM.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"function"},{"location":"interface/#Interface","page":"Interface","title":"Interface","text":"This page documents the Interface for user interaction.","category":"section"},{"location":"interface/#AdversarialAttacks.attack","page":"Interface","title":"AdversarialAttacks.attack","text":"attack(atk, model, sample; kwargs...)\n\nApply an adversarial attack to a sample using the given model.\n\nArguments\n\natk::AbstractAttack: The attack object to apply.\nmodel: The model to attack. Supports:\nChain (for white-box and black-box attacks)\nDecisionTreeClassifier (for black-box attacks)\nsample::AbstractArray{<:Number} or NamedTuple:\nRaw input array, or\nNamedTuple with data and label fields.\nkwargs...: Additional keyword arguments.\n\nReturns\n\nAdversarial sample produced by the attack.\n\nNotes\n\nWhiteBoxAttack is supported for Chain.\nBlackBoxAttack is supported for both Chain and DecisionTreeClassifier (treated as black-box models, using only model outputs).\n\n\n\n\n\n","category":"function"},{"location":"interface/#AdversarialAttacks.benchmark","page":"Interface","title":"AdversarialAttacks.benchmark","text":"benchmark(atk::AbstractAttack, model, dataset, metric::Function; kwargs...)\n\nEvaluate attack performance on a dataset with labels using a given metric.\n\nArguments\n\natk::AbstractAttack: Attack algorithm\nmodel: Target model to attack\ndataset: Dataset with samples and labels\nmetric::Function: Evaluation metric with signature metric(model, adv_samples, labels)\n\nReturns\n\nScalar metric value representing attack performance\n\n\n\n\n\n","category":"function"},{"location":"examples/blackbox_basicrandomsearch_decisiontree_iris/#Black-Box-–-Basic-Random-Search-(DecisionTree,-Iris)","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"Black-Box – Basic Random Search (DecisionTree, Iris)","text":"This tutorial demonstrates a black‑box adversarial attack using Basic Random Search on a decision tree trained on the Iris dataset. Unlike the MNIST example, the attack only relies on model queries and does not use gradients or internal parameters.","category":"section"},{"location":"examples/blackbox_basicrandomsearch_decisiontree_iris/#What-the-script-does","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"What the script does","text":"Trains a DecisionTreeClassifier on the four Iris features (sepal length/width, petal length/width) with a limited maximum depth.\nPicks a correctly classified sample and constructs a (data, label) pair compatible with the craft API.\nRuns a BasicRandomSearch attack with feature‑wise bounds, searching for a nearby point that reduces the true‑class probability and potentially changes the predicted class.","category":"section"},{"location":"examples/blackbox_basicrandomsearch_decisiontree_iris/#Visual-summary","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"Visual summary","text":"The resulting adversarial effect in feature space is summarized with:\n\n(Image: Basic Random Search on Iris)\n\nThe figure consists of two scatter plots:\n\nLeft (features 1 & 2): Sepal length vs sepal width, showing all Iris samples with different markers for setosa, versicolor, and virginica. In this projection the original and adversarial points almost overlap, because the attack changes features 1 and 2 only minimally.\nRight (features 3 & 4): Petal length vs petal width, again with class‑specific markers, and two highlighted points for the original sample and its adversarial counterpart.\n\nThe annotation in the right panel indicates the true class, the original prediction, and the adversarial prediction (for example, true: versicolor, original: versicolor, adversarial: virginica), making it clear how a small move in the feature space can cross the decision boundary of the tree in a purely query‑based setting.","category":"section"},{"location":"FGSM/#FGSM-(White-Box-Attack)","page":"FGSM (White-Box)","title":"FGSM (White-Box Attack)","text":"This page documents the Fast Gradient Sign Method (FGSM), a white-box adversarial attack that requires access to model gradients.","category":"section"},{"location":"FGSM/#FGSM-Implementation","page":"FGSM (White-Box)","title":"FGSM Implementation","text":"","category":"section"},{"location":"FGSM/#AdversarialAttacks.FGSM","page":"FGSM (White-Box)","title":"AdversarialAttacks.FGSM","text":"FGSM(; epsilon=0.1)\n\nA struct that can be used to create a white-box adversarial attack of type Fast Gradient Sign Method. Subtype of WhiteBoxAttack.\n\nArguments\n\nepsilon: Step size used to scale the sign of the gradient. Defaults to 0.1.\n\n\n\n\n\n","category":"type"},{"location":"FGSM/#AdversarialAttacks.craft-Tuple{Any, Flux.Chain, FGSM}","page":"FGSM (White-Box)","title":"AdversarialAttacks.craft","text":"craft(sample, model, attack::FGSM)\n\nPerform a Fast Gradient Sign Method (FGSM) white-box adversarial attack on the given model using the provided sample.\n\nArguments\n\nsample: Input sample as a named tuple with data and label.\nmodel::FluxModel: The machine learning (deep learning) model to be attacked.\nattack::FGSM: An instance of the FGSM.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"FGSM/#AdversarialAttacks.hyperparameters-Tuple{FGSM}","page":"FGSM (White-Box)","title":"AdversarialAttacks.hyperparameters","text":"hyperparameters(atk::FGSM) -> Dict{String,Any}\n\nReturn hyperparameters for an FGSM attack.\n\nReturns\n\nDict{String,Any}: Dictionary containing attack hyperparameters (e.g., epsilon).\n\n\n\n\n\n","category":"method"},{"location":"BasicRandomSearch/#BasicRandomSearch-(Black-Box-Attack)","page":"BasicRandomSearch (Black-Box)","title":"BasicRandomSearch (Black-Box Attack)","text":"This page documents the BasicRandomSearch algorithm (SimBA variant), a black-box adversarial attack that only requires query access to model predictions.","category":"section"},{"location":"BasicRandomSearch/#BasicRandomSearch-Implementation","page":"BasicRandomSearch (Black-Box)","title":"BasicRandomSearch Implementation","text":"","category":"section"},{"location":"BasicRandomSearch/#AdversarialAttacks.BasicRandomSearch","page":"BasicRandomSearch (Black-Box)","title":"AdversarialAttacks.BasicRandomSearch","text":"BasicRandomSearch(; epsilon=0.1, bounds=nothing, rng=Random.default_rng())\n\nSubtype of BlackBoxAttack. Creates adversarial examples using the SimBA random search algorithm.\n\nArguments\n\nepsilon: Step size for perturbations (default: 0.1)\nbounds: Optional vector of (lower, upper) tuples specifying per-feature bounds.           If nothing, defaults to [0, 1] for all features (suitable for normalized images).           For tabular data, provide bounds matching feature ranges, e.g.,           [(4.3, 7.9), (2.0, 4.4), ...] for Iris-like data.\nrng: Random number generator for reproducibility (default: Random.default_rng())\n\n\n\n\n\n","category":"type"},{"location":"BasicRandomSearch/#AdversarialAttacks.craft-Tuple{Any, Flux.Chain, BasicRandomSearch}","page":"BasicRandomSearch (Black-Box)","title":"AdversarialAttacks.craft","text":"craft(sample, model::Chain, attack::BasicRandomSearch)\n\nPerform a black-box adversarial attack on the given model using the provided sample using the Basic Random Search variant SimBA.\n\nArguments\n\nsample: The input sample to be changed.\nmodel::Chain: The machine learning (deep learning, classical machine learning) model to be attacked.\nattack::BasicRandomSearch: An instance of the BasicRandomSearch (BlackBox) attack.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"BasicRandomSearch/#AdversarialAttacks.craft-Tuple{Any, DecisionTree.DecisionTreeClassifier, BasicRandomSearch}","page":"BasicRandomSearch (Black-Box)","title":"AdversarialAttacks.craft","text":"craft(sample, model::DecisionTreeClassifier, attack::BasicRandomSearch)\n\nPerform a black-box adversarial attack on a DecisionTreeClassifier using BasicRandomSearch (SimBA).\n\nArguments\n\nsample: NamedTuple with data and label fields.\nmodel::DecisionTreeClassifier: DecisionTree.jl classifier to attack.\nattack::BasicRandomSearch: Attack instance with epsilon and optional bounds.\n\nReturns\n\nAdversarial example (same shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"examples/#Tutorials-and-Examples-Overview","page":"Overview","title":"Tutorials & Examples Overview","text":"The Tutorials & Examples section provides end‑to‑end, runnable scripts that demonstrate how to use AdversarialAttacks.jl in realistic workflows. Each example focuses on a concrete combination of model type, dataset, and attack method, and comes with its own project environment for reproducible execution.\n\nThe current examples cover:\n\nA white‑box FGSM attack against a Flux CNN on MNIST.\nA black‑box Basic Random Search attack against a DecisionTree classifier on the Iris dataset.\n\nWhen working from the package environment in a Julia REPL started in the repository root, you can activate the dedicated examples environment and run the scripts as follows:\n\njulia> using Pkg\njulia> Pkg.activate(\"./examples\")   # activate the examples environment from the repository root\njulia> include(\"examples/whitebox_fgsm_flux_mnist.jl\")\njulia> include(\"examples/blackbox_basicrandomsearch_decisiontree_iris.jl\")\n\nFrom the examples/ directory, all scripts can be run with:\n\njulia --project=. whitebox_fgsm_flux_mnist.jl\njulia --project=. blackbox_basicrandomsearch_decisiontree_iris.jl\n\nEach tutorial logs training and attack statistics and opens Plots.jl visualizations to inspect the resulting adversarial examples.","category":"section"},{"location":"#AdversarialAttacks.jl","page":"Getting Started","title":"AdversarialAttacks.jl","text":"(Image: Dev) (Image: Build Status) (Image: Coverage) (Image: License)\n\nAdversarialAttacks.jl is a lightweight Julia package for experimenting with adversarial attacks against neural networks and tree‑based models, focusing on FGSM (white‑box) and random‑search–based (black‑box) attacks.\n\nCurrently, this package supports only models implemented as Flux chains (neural networks) and decision trees from the DecisionTree.jl package. Support for other model types may be added in the future.","category":"section"},{"location":"#Installation","page":"Getting Started","title":"Installation","text":"You can install the package via the Julia package manager. In the Julia REPL, run:\n\njulia> ]add https://github.com/shnaky/AdversarialAttacks.jl","category":"section"},{"location":"#Examples","page":"Getting Started","title":"Examples","text":"The following example shows how to create an adversarial sample from a single input sample using the FGSM attack:","category":"section"},{"location":"#FGSM-attack-Flux-Integration-(white-box)","page":"Getting Started","title":"FGSM attack - Flux Integration (white-box)","text":"julia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(\n           Dense(2, 2, tanh),\n           Dense(2, 2),\n           softmax,\n       )\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> sample = (data=rand(Float32, 2, 1), label=Flux.onehot(1, 1:2) )\n\njulia> adv_sample = attack(fgsm, model, sample)","category":"section"},{"location":"#BasicRandomSearch-attack-DecisionTree-integration-(black-box)","page":"Getting Started","title":"BasicRandomSearch attack - DecisionTree integration (black-box)","text":"julia> using AdversarialAttacks\n\njulia> using DecisionTree\n\njulia> classes = [1, 2, 3]\n\njulia> X = rand(24, 4) .* 4\n\njulia> y = vcat(\n    fill(classes[1], 8),\n    fill(classes[2], 8),\n    fill(classes[3], 8),\n)\n\njulia> tree = DecisionTreeClassifier(; classes = classes)\n\njulia> fit!(tree, X, y)\n\njulia> sample = (data = X[:, 1], label = y[1])\n\njulia> atk = BasicRandomSearch(epsilon = 0.1f0)\n\njulia> x_adv = attack(atk, tree, sample)","category":"section"},{"location":"#Batch-Example","page":"Getting Started","title":"Batch Example","text":"You can also apply an attack to a batch of samples represented as a tensor.\n\njulia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(Dense(4, 8, relu), Dense(8, 2), softmax)\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> X = rand(Float32, 4, 3)  # 3 samples, 4 features each\n\njulia> Y = Flux.onehotbatch([1, 2, 1], 1:2)  # labels for each sample\n\njulia> tensor = (data=X, label=Y)\n\njulia> adv_samples = attack(fgsm, model, tensor)","category":"section"},{"location":"#Evaluation-Example","page":"Getting Started","title":"Evaluation Example","text":"Get an evaluation report on your adversarial attack.\n\njulia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(Dense(4, 3), softmax)\n\njulia> test_data = [ (data=randn(Float32, 4), label=Flux.onehot(rand(1:3), 1:3)) for _ in 1:10 ]\n\njulia> fgsm = FGSM(epsilon=0.5)\n\njulia> report = evaluate_robustness(model, fgsm, test_data)\n\njulia> println(report)\n=== Robustness Evaluation Report ===\n\nDataset\n  Total samples evaluated        : 10\n  Clean-correct samples          : 4 / 10\n\nClean Performance\n  Clean accuracy                 : 40.0%\n\nAdversarial Performance\n  Adversarial accuracy           : 0.0%\n\nAttack Effectiveness\n  Successful attacks             : 4 / 4\n  Attack success rate (ASR)      : 100.0%\n  Robustness score (1 - ASR)     : 0.0%\n\nPerturbation Analysis (L_inf norm)\n  Maximum perturbation           : 0.5\n  Mean perturbation              : 0.5\n\nNotes\n  • Attack success is counted only when:\n    - the clean prediction is correct\n    - the adversarial prediction is incorrect\n===================================","category":"section"},{"location":"#License","page":"Getting Started","title":"License","text":"This package is licensed under the MIT License. See LICENSE for details.","category":"section"},{"location":"#AI-Assistance","page":"Getting Started","title":"AI Assistance","text":"This project uses AI coding assistants for routine maintenance tasks such as:\n\nCode style fixes and documentation improvements\nConsolidating imports and minor refactoring\nAddressing code review feedback\n\nAll AI-generated changes are reviewed before merging.","category":"section"},{"location":"evaluation/#Robustness-Evaluation-Suite","page":"Robustness Evaluation Suite","title":"Robustness Evaluation Suite","text":"This page documents Robustness Evaluation Suite.","category":"section"},{"location":"evaluation/#AdversarialAttacks.RobustnessReport","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.RobustnessReport","text":"RobustnessReport\n\nReport on model robustness against an adversarial attack. Printing a RobustnessReport (via println(report)) displays a nicely formatted summary including clean/adversarial accuracy, attack success rate, and robustness score.\n\nFields\n\nnum_samples::Int: Total samples evaluated\nnum_clean_correct::Int: Samples correctly classified before attack\nclean_accuracy::Float64: Accuracy on clean samples\nadv_accuracy::Float64: Accuracy on adversarial samples\nattack_success_rate::Float64: (ASR) Fraction of successful attacks (on correctly classified samples)\nrobustness_score::Float64: 1.0 - attacksuccessrate (ASR)\nnum_successful_attacks::Int: Number of successful attacks\nlinf_norm_max::Float64: Maximum L_inf norm of perturbations across all samples\nlinf_norm_mean::Float64: Mean L_inf norm of perturbations across all samples\n\nNote\n\nAn attack succeeds when the clean prediction is correct but the adversarial prediction is incorrect. The L_inf norm measures the maximum absolute change in any feature of the input.\n\n\n\n\n\n","category":"type"},{"location":"evaluation/#AdversarialAttacks.evaluate_robustness-Tuple{Any, Any, Any}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.evaluate_robustness","text":"evaluate_robustness(model, attack, test_data; num_samples=100)\n\nEvaluate model robustness by running attack on multiple samples.\n\nArguments\n\nmodel: The model to evaluate.\nattack: The attack to use.\ntest_data: Collection of test samples.\nnum_samples::Int=100: Number of samples to test. If more than available samples, uses all available samples.\n\nReturns\n\nRobustnessReport: Report containing accuracy, attack success rate, and robustness metrics\n\nExample\n\nreport = evaluate_robustness(model, FGSM(ε=0.1), test_data, num_samples=50)\nprintln(report)\n\n\n\n\n\n","category":"method"}]
}
