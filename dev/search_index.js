var documenterSearchIndex = {"docs":
[{"location":"fgsm/#Fast-Gradient-Sign-Method-Attack","page":"Fast Gradient Sign Method Attack","title":"Fast Gradient Sign Method Attack","text":"This page documents the Fast Gradient Sign Method.","category":"section"},{"location":"fgsm/#FGSM-Implementation","page":"Fast Gradient Sign Method Attack","title":"FGSM Implementation","text":"","category":"section"},{"location":"fgsm/#AdversarialAttacks.FGSM","page":"Fast Gradient Sign Method Attack","title":"AdversarialAttacks.FGSM","text":"FGSM(; epsilon=0.1)\n\nA struct that can be used to create a white-box adversarial attack of type Fast Gradient Sign Method. Subtype of WhiteBoxAttack.\n\nArguments\n\nepsilon: Step size used to scale the sign of the gradient. Defaults to 0.1.\n\n\n\n\n\n","category":"type"},{"location":"fgsm/#AdversarialAttacks.craft-Tuple{Any, DifferentiableModel, FGSM}","page":"Fast Gradient Sign Method Attack","title":"AdversarialAttacks.craft","text":"craft(sample, model, attack::FGSM)\n\nPerforms a Fast Gradient Sign Method (FGSM) white-box adversarial attack on the given model using the provided sample.\n\nArguments\n\nsample: Input sample as a named tuple with data and label.\nmodel::DifferentiableModel: The machine learning (deep learning) model to be attacked.\nattack::FGSM: An instance of the FGSM.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"fgsm/#AdversarialAttacks.hyperparameters-Tuple{FGSM}","page":"Fast Gradient Sign Method Attack","title":"AdversarialAttacks.hyperparameters","text":"hyperparameters(atk::FGSM) -> Dict{String,Any}\n\nReturn hyperparameters for an FGSM attack.\n\nReturns\n\nDict{String,Any}: Dictionary containing attack hyperparameters (e.g., epsilon).\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#Attack-Interface","page":"Attack Interface","title":"Attack Interface","text":"This page documents the shared attack abstractions.","category":"section"},{"location":"attack_interface/#AdversarialAttacks.AbstractAttack","page":"Attack Interface","title":"AdversarialAttacks.AbstractAttack","text":"Abstract supertype for all adversarial attacks.\n\nExpected interface (to be implemented per concrete attack):\n\nname(::AbstractAttack)::String\nhyperparameters(::AbstractAttack)::Dict{String,Any}\ncraft(sample, model, atk::AbstractAttack; kwargs...) returning an adversarial example\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.WhiteBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.WhiteBoxAttack","text":"Abstract type for white-box adversarial attacks.\n\nWhite-box attacks have full access to the model's internals, including gradients, weights, and architecture. This enables the use of gradient-based optimization and other techniques to craft adversarial examples.\n\nUse this type when the attacker can inspect and manipulate the model's internal parameters and computations. If only input-output access is available, use BlackBoxAttack instead.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.BlackBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.BlackBoxAttack","text":"Abstract type for black-box adversarial attacks.\n\nBlack-box attacks only have access to the model's input-output behavior, without knowledge of the model's internals, gradients, or architecture. These attacks typically rely on query-based methods (e.g., optimization via repeated queries) or transferability from surrogate models.\n\nUse BlackBoxAttack when you do not have access to the model's internal parameters or gradients, such as in deployed systems or APIs.  In contrast, use WhiteBoxAttack when you have full access to the model's internals and can leverage gradient information for crafting adversarial examples.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.name-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.name","text":"name(atk::AbstractAttack) -> String\n\nHuman-readable name for an attack.\n\nReturns\n\nString: String representation of the attack type.\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#AdversarialAttacks.hyperparameters-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.hyperparameters","text":"hyperparameters(atk::AbstractAttack) -> Dict{String,Any}\n\nReturn hyperparameters for an attack.\n\nReturns\n\nDict{String,Any}: Dictionary of attack hyperparameters.\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#AdversarialAttacks.craft","page":"Attack Interface","title":"AdversarialAttacks.craft","text":"craft(sample, model, attack::AbstractAttack; kwargs...) -> adversarial_sample\n\nCraft an adversarial example by applying the attack to a sample.\n\nArguments\n\nsample: Input sample to perturb (e.g., image, text)\nmodel: Target model to attack\nattack::AbstractAttack: Attack configuration and algorithm\nkwargs...: Additional attack-specific parameters\n\nReturns\n\nAdversarial example with the same shape as the input sample\n\n\n\n\n\ncraft(sample, model::AbstractModel, attack::BasicRandomSearch)\n\nPerforms a black-box adversarial attack on the given model using the provided sample using the Basic Random Search variant SimBA.\n\nArguments\n\nsample: The input sample to be changed.\nmodel::AbstractModel: The machine learning (deep learning, classical machine learning) model to be attacked.\nattack::BasicRandomSearch: An instance of the BasicRandomSearch (BlackBox) attack.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\ncraft(sample, model::DecisionTreeClassifier, attack::BasicRandomSearch)\n\nPerforms a black-box adversarial attack on a DecisionTreeClassifier using BasicRandomSearch (SimBA).\n\nArguments\n\nsample: NamedTuple with data and label fields.\nmodel::DecisionTreeClassifier: DecisionTree.jl classifier to attack.\nattack::BasicRandomSearch: Attack instance with epsilon and optional bounds.\n\nReturns\n\nAdversarial example (same shape as sample.data).\n\n\n\n\n\ncraft(sample, model, attack::SquareAttack)\n\nPerforms a black-box adversarial attack on the given model using the provided sample using the SquareAttack algorithm.\n\nArguments\n\nsample: The input sample to be changed.\nmodel::AbstractModel: The machine learning (deep learning, classical machine learning) model to be attacked.\nattack::SquareAttack: An instance of the SquareAttack (BlackBox) attack.\n\nReturns\n\nAdversarial example (same type and shape as sample).\n\n\n\n\n\ncraft(sample, model, attack::FGSM)\n\nPerforms a Fast Gradient Sign Method (FGSM) white-box adversarial attack on the given model using the provided sample.\n\nArguments\n\nsample: Input sample as a named tuple with data and label.\nmodel::DifferentiableModel: The machine learning (deep learning) model to be attacked.\nattack::FGSM: An instance of the FGSM.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"function"},{"location":"interface/#Interface","page":"Interface","title":"Interface","text":"This page documents the Interface for user interaction.","category":"section"},{"location":"interface/#AdversarialAttacks.attack","page":"Interface","title":"AdversarialAttacks.attack","text":"attack(atk, model, sample; kwargs...)\n\nApply an adversarial attack to a sample using the given model.\n\nArguments\n\natk::AbstractAttack: The attack object to apply.\nmodel: The model to attack. Supports:\nAbstractModel subtypes (library-defined model wrappers)\nNative models like DecisionTreeClassifier (for black-box attacks)\nsample::AbstractArray{<:Number} or NamedTuple: Input sample (array) or named tuple with data and label fields.\nkwargs...: Additional keyword arguments.\n\nReturns\n\nAdversarial sample produced by the attack.\n\nNotes\n\nWhiteBoxAttack requires a DifferentiableModel.\nBlackBoxAttack works for any AbstractModel or supported native models.\n\n\n\n\n\n","category":"function"},{"location":"interface/#AdversarialAttacks.benchmark","page":"Interface","title":"AdversarialAttacks.benchmark","text":"benchmark(atk::AbstractAttack, model::AbstractModel, dataset, metric::Function)\n\nEvaluate attack performance on a dataset with labels using a given metric.\n\nArguments\n\natk::AbstractAttack: Attack algorithm\nmodel::AbstractModel: Target model to attack\ndataset: Dataset with samples and labels\nmetric::Function: Evaluation metric with signature metric(model, adv_samples, labels)\n\nReturns\n\nScalar metric value representing attack performance\n\n\n\n\n\n","category":"function"},{"location":"model_interface/#Model-Interface","page":"Model Interface","title":"Model Interface","text":"","category":"section"},{"location":"model_interface/#AdversarialAttacks.AbstractModel","page":"Model Interface","title":"AdversarialAttacks.AbstractModel","text":"Abstract base for all models that can be attacked.\n\nMust implement: predict(model, x), loss(model, x, y). Optional: params(model) for white-box attacks.\n\n\n\n\n\n","category":"type"},{"location":"model_interface/#AdversarialAttacks.DifferentiableModel","page":"Model Interface","title":"AdversarialAttacks.DifferentiableModel","text":"Models that support gradient-based white-box attacks (e.g. neural networks with Flux.jl).\n\n\n\n\n\n","category":"type"},{"location":"model_interface/#AdversarialAttacks.NonDifferentiableModel","page":"Model Interface","title":"AdversarialAttacks.NonDifferentiableModel","text":"Models without gradient access for black-box attacks (e.g. traditional ML models such as decision tree, SVM, logistic regression).\n\n\n\n\n\n","category":"type"},{"location":"model_interface/#AdversarialAttacks.FluxModel","page":"Model Interface","title":"AdversarialAttacks.FluxModel","text":"FluxModel(model::Flux.Chain)\n\nFlux-based differentiable model wrapper.\n\nAllows using Flux.Chain with the DifferentiableModel interface.\n\nArguments\n\nmodel::Flux.Chain: Flux chain model to wrap.\n\nExamples\n\nchain = Chain(Dense(10 => 5), Dense(5 => 2))\nmodel = FluxModel(chain)\n\n\n\n\n\n","category":"type"},{"location":"model_interface/#AdversarialAttacks.name-Tuple{AbstractModel}","page":"Model Interface","title":"AdversarialAttacks.name","text":"name(m::AbstractModel) -> String\n\nReturn a human-readable name for the model.\n\nArguments\n\nm::AbstractModel: Model instance.\n\nReturns\n\nString: Descriptive name of the model type (e.g. \"FluxModel\", \"TreeModel\").\n\nExamples\n\nname(FluxModel(chain)) # \"FluxModel\"\n\n\n\n\n\n","category":"method"},{"location":"model_interface/#AdversarialAttacks.name-Tuple{FluxModel}","page":"Model Interface","title":"AdversarialAttacks.name","text":"name(m::FluxModel)\n\nReturn a human-readable name for the Flux model.\n\nReturns\n\nString: \"FluxModel\"\n\n\n\n\n\n","category":"method"},{"location":"model_interface/#AdversarialAttacks.predict","page":"Model Interface","title":"AdversarialAttacks.predict","text":"predict(m::AbstractModel, x) -> y\n\nCompute model predictions for input x.\n\nFor classifiers, this usually returns logits or class probabilities.\n\nArguments\n\nm::AbstractModel: Model instance.\nx: Input data (e.g. feature vector, image batch).\n\nReturns\n\ny: Model output with a type and shape defined by the concrete model, usually matching the shape expected by loss.\n\nExamples\n\nŷ = predict(model, x_batch)\n\n\n\n\n\npredict(m::FluxModel, x)\n\nForward pass: delegate to the wrapped Flux.Chain.\n\nArguments\n\nm::FluxModel: FluxModel instance\nx: input data\n\nReturns\n\nmodel(x): Flux chain output\n\n\n\n\n\n","category":"function"},{"location":"model_interface/#AdversarialAttacks.loss","page":"Model Interface","title":"AdversarialAttacks.loss","text":"loss(m::AbstractModel, x, y) -> Real\n\nCompute a scalar loss for input–target pair (x, y).\n\nWhite-box attacks will typically differentiate this loss with respect to the input x.\n\nArguments\n\nm::AbstractModel: Model instance.\nx: Input data.\ny: Target labels (e.g. one-hot vectors or class indices).\n\nReturns\n\nReal: Scalar loss value used for training or attacks.\n\nExamples\n\nℓ = loss(model, x_batch, y_batch)\n\n\n\n\n\nloss(m::FluxModel, x, y)\n\nCross-entropy loss for classification tasks.\n\nArguments\n\nm::FluxModel: FluxModel instance\nx: input data\ny: ground truth labels (one-hot or class indices)\n\nReturns\n\nFloat32: logitcrossentropy loss value\n\n\n\n\n\n","category":"function"},{"location":"model_interface/#AdversarialAttacks.params","page":"Model Interface","title":"AdversarialAttacks.params","text":"params(m::AbstractModel)\n\nReturn the trainable parameters of m, if available.\n\nWhite-box attacks may use this; black-box models can ignore it.\n\nArguments\n\nm::AbstractModel: Model instance.\n\nReturns\n\nImplementation-defined: Typically a collection of arrays or a parameter container (e.g. Flux.Params for FluxModel).\n\nExamples\n\nθ = params(model)\n\n\n\n\n\nparams(m::FluxModel)\n\nReturn all trainable parameters of the wrapped Flux model. White-box attacks may use this; black-box models can ignore it.\n\nArguments\n\nm::FluxModel: FluxModel instance\n\nReturns\n\nFlux.Params: collection of trainable parameters\n\nExamples\n\nθ = params(model)\ngrads = gradient(θ) do\n    loss(model, x, y)\nend\n\n\n\n\n\n","category":"function"},{"location":"blackbox_subtypes/#Black-Box-Algorithms","page":"Black Box Attacks","title":"Black Box Algorithms","text":"This page documents the black box algorithms.","category":"section"},{"location":"blackbox_subtypes/#AdversarialAttacks.BasicRandomSearch","page":"Black Box Attacks","title":"AdversarialAttacks.BasicRandomSearch","text":"BasicRandomSearch(; epsilon=0.1, bounds=nothing)\n\nSubtype of BlackBoxAttack. Creates adversarial examples using the SimBA random search algorithm.\n\nArguments\n\nepsilon: Step size for perturbations (default: 0.1)\nbounds: Optional vector of (lower, upper) tuples specifying per-feature bounds.           If nothing, defaults to [0, 1] for all features (suitable for normalized images).           For tabular data, provide bounds matching feature ranges, e.g.,           [(4.3, 7.9), (2.0, 4.4), ...] for Iris-like data.\n\n\n\n\n\n","category":"type"},{"location":"blackbox_subtypes/#AdversarialAttacks.SquareAttack","page":"Black Box Attacks","title":"AdversarialAttacks.SquareAttack","text":"SquareAttack(parameters::Dict=Dict{String,Any}())\n\nSubtype of BlackBoxAttack. Can be used to create an adversarial example in the black-box setting using the square attack algorithm.\n\nArguments\n\nparameters: can be used to pass attack parameters as a dict\n\n\n\n\n\n","category":"type"},{"location":"blackbox_subtypes/#AdversarialAttacks.craft-Tuple{Any, AbstractModel, BasicRandomSearch}","page":"Black Box Attacks","title":"AdversarialAttacks.craft","text":"craft(sample, model::AbstractModel, attack::BasicRandomSearch)\n\nPerforms a black-box adversarial attack on the given model using the provided sample using the Basic Random Search variant SimBA.\n\nArguments\n\nsample: The input sample to be changed.\nmodel::AbstractModel: The machine learning (deep learning, classical machine learning) model to be attacked.\nattack::BasicRandomSearch: An instance of the BasicRandomSearch (BlackBox) attack.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"blackbox_subtypes/#AdversarialAttacks.craft-Tuple{Any, AbstractModel, SquareAttack}","page":"Black Box Attacks","title":"AdversarialAttacks.craft","text":"craft(sample, model, attack::SquareAttack)\n\nPerforms a black-box adversarial attack on the given model using the provided sample using the SquareAttack algorithm.\n\nArguments\n\nsample: The input sample to be changed.\nmodel::AbstractModel: The machine learning (deep learning, classical machine learning) model to be attacked.\nattack::SquareAttack: An instance of the SquareAttack (BlackBox) attack.\n\nReturns\n\nAdversarial example (same type and shape as sample).\n\n\n\n\n\n","category":"method"},{"location":"#AdversarialAttacks.jl","page":"Getting Started","title":"AdversarialAttacks.jl","text":"(Image: Dev) (Image: Build Status) (Image: Coverage) (Image: License)","category":"section"},{"location":"#Installation","page":"Getting Started","title":"Installation","text":"You can install the package via the Julia package manager. In the Julia REPL, run:\n\njulia> ]add https://github.com/shnaky/AdversarialAttacks.jl","category":"section"},{"location":"#Examples","page":"Getting Started","title":"Examples","text":"[!WARNING] This project is still in early development. The attack algorithms have not been fully implemented yet.\n\nThe following example shows how to create an adversarial sample from a single input sample using the FGSM attack:\n\njulia> using AdversarialAttacks\n\njulia> struct MyModel <: DifferentiableModel end\n\njulia> AdversarialAttacks.predict(::MyModel, x) = sigmoid.(sum(x, dims=1))\n\njulia> AdversarialAttacks.loss(::MyModel, x, y) = Flux.binarycrossentropy(predict(MyModel(), x), y)\n\njulia> AdversarialAttacks.params(::MyModel) = Flux.Params([])\n\njulia> model = MyModel()\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> sample = (data=rand(Float32, 10, 1), label=1)\n\njulia> adv_sample = attack(fgsm, model, sample)","category":"section"},{"location":"#Flux-Integration","page":"Getting Started","title":"Flux Integration","text":"This package can also work with Flux.jl models. Wrap your Flux model using the FluxModel interface:\n\njulia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> m = Chain(Dense(2, 2, tanh), Dense(2, 2))\n\njulia> model = FluxModel(m)\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> sample = (data=rand(Float32, 2, 1), label=Flux.onehot(1, 1:2) )\n\njulia> adv_sample = attack(fgsm, model, sample)","category":"section"},{"location":"#Batch-Example","page":"Getting Started","title":"Batch Example","text":"You can also apply an attack to a batch of samples represented as a tensor.\n\njulia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> m = Chain(Dense(4, 8, relu), Dense(8, 2))\n\njulia> model = FluxModel(m)\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> X = rand(Float32, 4, 3)  # 3 samples, 4 features each\n\njulia> Y = Flux.onehotbatch([1, 2, 1], 1:2)  # labels for each sample\n\njulia> tensor = (data=X, label=Y)\n\njulia> adv_samples = attack(fgsm, model, tensor)","category":"section"},{"location":"#Evaluation-Example","page":"Getting Started","title":"Evaluation Example","text":"Get an evaluation report on your adversarial attack.\n\njulia> using AdversarialAttacks, Flux\n\njulia> using Flux\n\njulia> model_flux = Chain(Dense(4, 3), softmax)\n\njulia> model = FluxModel(model_flux)\n\njulia> test_data = [ (data=randn(Float32, 4), label=Flux.onehot(rand(1:3), 1:3)) for _ in 1:10 ]\n\njulia> attack = FGSM(epsilon=0.3)\n\njulia> report = evaluate_robustness(model, attack, test_data)\n\njulia> println(report)\n=== Robustness Evaluation Report ===\n\nDataset\n  Total samples evaluated        : 10\n  Clean-correct samples          : 4 / 10\n\nClean Performance\n  Clean accuracy                 : 40.0%\n\nAdversarial Performance\n  Adversarial accuracy           : 30.0%\n\nAttack Effectiveness\n  Successful attacks             : 1 / 4\n  Attack success rate (ASR)      : 25.0%\n  Robustness score (1 - ASR)     : 75.0%\n\nNotes\n  • Attack success is counted only when:\n    - the clean prediction is correct\n    - the adversarial prediction is incorrect\n===================================","category":"section"},{"location":"#License","page":"Getting Started","title":"License","text":"This package is licensed under the MIT License. See LICENSE for details.","category":"section"},{"location":"evaluation/#Robustness-Evaluation-Suite","page":"Robustness Evaluation Suite","title":"Robustness Evaluation Suite","text":"This page documents Robustness Evaluation Suite.","category":"section"},{"location":"evaluation/#AdversarialAttacks.RobustnessReport","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.RobustnessReport","text":"RobustnessReport\n\nReport on model robustness against an adversarial attack. Printing a RobustnessReport (via println(report)) displays a nicely formatted summary including clean/adversarial accuracy, attack success rate, and robustness score.\n\nFields\n\nnum_samples::Int: Total samples evaluated\nnum_clean_correct::Int: Samples correctly classified before attack\nclean_accuracy::Float64: Accuracy on clean samples\nadv_accuracy::Float64: Accuracy on adversarial samples\nattack_success_rate::Float64: (ASR) Fraction of successful attacks (on correctly classified samples)\nrobustness_score::Float64: 1.0 - attacksuccessrate (ASR)\nnum_successful_attacks::Int: Number of successful attacks\n\nNote\n\nAn attack succeeds when the clean prediction is correct but the adversarial prediction is incorrect.\n\n\n\n\n\n","category":"type"},{"location":"evaluation/#AdversarialAttacks.evaluate_robustness-Tuple{Any, Any, Any}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.evaluate_robustness","text":"evaluate_robustness(model, attack, test_data; num_samples=100)\n\nEvaluate model robustness by running attack on multiple samples.\n\nArguments\n\nmodel: The model to evaluate.\nattack: The attack to use.\ntest_data: Collection of test samples.\nnum_samples::Int=100: Number of samples to test. If more than available samples, uses all available samples.\n\nReturns\n\nRobustnessReport: Report containing accuracy, attack success rate, and robustness metrics\n\nExample\n\nreport = evaluate_robustness(model, FGSM(ε=0.1), test_data, num_samples=50)\nprintln(report)\n\n\n\n\n\n","category":"method"}]
}
