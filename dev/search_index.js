var documenterSearchIndex = {"docs":
[{"location":"attack_interface/#Attack-Interface","page":"Attack Interface","title":"Attack Interface","text":"This page documents the shared attack abstractions.","category":"section"},{"location":"attack_interface/#Type-Hierarchy","page":"Attack Interface","title":"Type Hierarchy","text":"using AdversarialAttacks\n\nprintln(\"FGSM <: WhiteBoxAttack:       \", FGSM <: WhiteBoxAttack)\nprintln(\"WhiteBoxAttack <: AbstractAttack: \", WhiteBoxAttack <: AbstractAttack)\nprintln(\"BasicRandomSearch <: BlackBoxAttack: \", BasicRandomSearch <: BlackBoxAttack)\nprintln(\"BlackBoxAttack <: AbstractAttack:    \", BlackBoxAttack <: AbstractAttack)","category":"section"},{"location":"attack_interface/#AdversarialAttacks.AbstractAttack","page":"Attack Interface","title":"AdversarialAttacks.AbstractAttack","text":"Abstract supertype for all adversarial attacks.\n\nExpected interface (to be implemented per concrete attack):\n\nname(::AbstractAttack)::String\nattack(atk::AbstractAttack, model, sample; kwargs...) returning an adversarial example\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.WhiteBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.WhiteBoxAttack","text":"Abstract type for white-box adversarial attacks.\n\nWhite-box attacks have full access to the model's internals, including gradients, weights, and architecture. This enables the use of gradient-based optimization and other techniques to craft adversarial examples.\n\nUse this type when the attacker can inspect and manipulate the model's internal parameters and computations. If only input-output access is available, use BlackBoxAttack instead.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.BlackBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.BlackBoxAttack","text":"Abstract type for black-box adversarial attacks.\n\nBlack-box attacks only have access to the model's input-output behavior, without knowledge of the model's internals, gradients, or architecture. These attacks typically rely on query-based methods (e.g., optimization via repeated queries) or transferability from surrogate models.\n\nUse BlackBoxAttack when you do not have access to the model's internal parameters or gradients, such as in deployed systems or APIs.  In contrast, use WhiteBoxAttack when you have full access to the model's internals and can leverage gradient information for crafting adversarial examples.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.name-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.name","text":"name(atk::AbstractAttack) -> String\n\nHuman-readable name for an attack.\n\nReturns\n\nString: String representation of the attack type.\n\n\n\n\n\n","category":"method"},{"location":"interface/#Interface","page":"Interface","title":"Interface","text":"This page documents the Interface for user interaction.","category":"section"},{"location":"interface/#Quick-Example","page":"Interface","title":"Quick Example","text":"using AdversarialAttacks\n\n# Construct attacks via the high-level API types\nfgsm = FGSM(epsilon = 0.01f0)\nbsr = BasicRandomSearch(epsilon = 0.1f0, bounds = [(0f0, 1f0)], max_iter = 10)\n\nprintln(\"FGSM: \", name(fgsm))\nprintln(\"BSR:  \", name(bsr))","category":"section"},{"location":"interface/#AdversarialAttacks.attack","page":"Interface","title":"AdversarialAttacks.attack","text":"attack(atk::AbstractAttack, model, sample; kwargs...) -> adversarial_sample\n\nGenerate an adversarial example by applying the attack to a sample.\n\nArguments\n\natk::AbstractAttack: Attack configuration and algorithm\nmodel: Target model to attack\nsample: Input sample to perturb (e.g., image, text)\nkwargs...: Additional attack-specific parameters\n\nReturns\n\nAdversarial example with the same shape as the input sample\n\n\n\n\n\nattack(atk::BasicRandomSearch, model::Chain, sample)\n\nPerform a black-box adversarial attack on the given model using the provided sample using the Basic Random Search variant SimBA.\n\nArguments\n\natk::BasicRandomSearch: An instance of the BasicRandomSearch (BlackBox) attack.\nmodel::Chain: The machine learning (deep learning, classical machine learning) model to be attacked.\nsample: The input sample to be changed.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\nattack(atk::BasicRandomSearch, model::DecisionTreeClassifier, sample)\n\nPerform a black-box adversarial attack on a DecisionTreeClassifier using BasicRandomSearch (SimBA).\n\nArguments\n\natk::BasicRandomSearch: Attack instance with epsilon and optional bounds.\nmodel::DecisionTreeClassifier: DecisionTree.jl classifier to attack.\nsample: NamedTuple with data and label fields.\n\nReturns\n\nAdversarial example (same shape as sample.data).\n\n\n\n\n\nattack(atk::BasicRandomSearch, mach::Machine, sample)\n\nBlack-box adversarial attack on an MLJ Machine (e.g. a RandomForestClassifier) using BasicRandomSearch (SimBA), via predict.\n\natk::BasicRandomSearch: Attack instance with epsilon and max_iter.\nmach::Machine: Trained MLJ machine with probabilistic predictions.\nsample: NamedTuple with data (feature vector) and label (true class index, 1-based).\n\nReturns an adversarial example with the same shape as sample.data.\n\n\n\n\n\nattack(atk::FGSM, model, sample)\n\nPerform a Fast Gradient Sign Method (FGSM) white-box adversarial attack on the given model using the provided sample.\n\nArguments\n\natk::FGSM: An instance of the FGSM.\nmodel::FluxModel: The machine learning (deep learning) model to be attacked.\nsample: Input sample as a named tuple with data and label.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"function"},{"location":"interface/#AdversarialAttacks.benchmark","page":"Interface","title":"AdversarialAttacks.benchmark","text":"benchmark(atk::AbstractAttack, model, dataset, metric::Function; kwargs...)\n\nEvaluate attack performance on a dataset with labels using a given metric.\n\nArguments\n\natk::AbstractAttack: Attack algorithm\nmodel: Target model to attack\ndataset: Dataset with samples and labels\nmetric::Function: Evaluation metric with signature metric(model, adv_samples, labels)\n\nReturns\n\nScalar metric value representing attack performance\n\n\n\n\n\n","category":"function"},{"location":"tutorials/whitebox_fgsm_flux_mnist/#White-Box-FGSM-Attack-on-MNIST-(Flux)","page":"White-Box – FGSM (Flux, MNIST)","title":"White-Box FGSM Attack on MNIST (Flux)","text":"This tutorial demonstrates how to perform a white-box adversarial attack using the Fast Gradient Sign Method (FGSM) against a small CNN trained on MNIST.\n\nWhat you will learn:\n\nHow to train a simple CNN with Flux on MNIST\nHow to construct an FGSM attack with AdversarialAttacks.jl\nHow to evaluate whether the attack succeeded\nHow to visualize original vs adversarial images","category":"section"},{"location":"tutorials/whitebox_fgsm_flux_mnist/#Prerequisites","page":"White-Box – FGSM (Flux, MNIST)","title":"Prerequisites","text":"Make sure you have the following packages installed: Flux, MLDatasets, OneHotArrays, Plots, and AdversarialAttacks.\n\nusing Random\nusing Flux\nusing OneHotArrays\nusing AdversarialAttacks\nusing MLDatasets\nusing Plots\n\nRandom.seed!(1234)\nprintln(\"=== White-Box FGSM Attack Tutorial ===\\n\")","category":"section"},{"location":"tutorials/whitebox_fgsm_flux_mnist/#1.-Load-MNIST-subset","page":"White-Box – FGSM (Flux, MNIST)","title":"1. Load MNIST subset","text":"We load a subset of MNIST (6000 samples) and reshape it into the 4D tensor format that Flux CNNs expect: (height, width, channels, batch). MLDatasets returns pixel values already in the [0, 1] range.\n\ntrain_x, train_y = MLDatasets.MNIST.traindata()        # 28×28×60000, Vector{Int}\ntrain_x = train_x[:, :, 1:6000]                        # use 6000 samples for speed\ntrain_y = train_y[1:6000]\n\nX = Float32.(reshape(train_x, 28, 28, 1, :))     # 28×28×1×N\ny = Flux.onehotbatch(train_y, 0:9)                      # 10×N one-hot labels","category":"section"},{"location":"tutorials/whitebox_fgsm_flux_mnist/#2.-Define-and-train-a-CNN","page":"White-Box – FGSM (Flux, MNIST)","title":"2. Define and train a CNN","text":"We define a small LeNet-style CNN with two convolutional layers followed by three dense layers, ending with a softmax output. The model is trained for 5 epochs using the Adam optimizer with cross-entropy loss.\n\nmodel = Chain(\n    Conv((5, 5), 1 => 6, relu, pad = 2), x -> maxpool(x, (2, 2)),  # 28 → 28 → 14\n    Conv((5, 5), 6 => 16, relu, pad = 0), x -> maxpool(x, (2, 2)), # 14 → 10 → 5\n    Flux.flatten,                                                 # 16*5*5 = 400\n    Dense(400, 120, relu),\n    Dense(120, 84, relu),\n    Dense(84, 10),\n    softmax,\n)\n\nloss(m, x, y) = Flux.crossentropy(m(x), y)\nopt = Flux.setup(Adam(0.001), model)\n\nprintln(\"Training for 5 epochs on mini-batches of size 128...\")\nbatch_size = 128\ndataloader = [\n    (\n            X[:, :, :, i:min(end, i + batch_size - 1)],\n            y[:, i:min(end, i + batch_size - 1)],\n        )\n        for i in 1:batch_size:size(X, 4)\n]\n\nfor epoch in 1:5\n    for (x_batch, y_batch) in dataloader\n        gs = gradient(m -> loss(m, x_batch, y_batch), model)\n        Flux.update!(opt, model, gs[1])\n    end\nend\n\nCheck training accuracy as a sanity check:\n\nfunction eval_acc(model, X_test, y_test)\n    correct = 0\n    for i in 1:size(X_test, 4)\n        pred_probs = model(X_test[:, :, :, i:i])\n        pred_label = argmax(pred_probs)[1]\n        true_label = argmax(y_test[:, i])\n        correct += (pred_label == true_label)\n    end\n    return correct / size(X_test, 4)\nend\n\nprintln(\"Train-subset acc: $(round(eval_acc(model, X, y) * 100, digits = 2))%\")\nprintln(\"✓ Trained simple CNN on MNIST subset\\n\")","category":"section"},{"location":"tutorials/whitebox_fgsm_flux_mnist/#3.-Pick-a-demo-sample","page":"White-Box – FGSM (Flux, MNIST)","title":"3. Pick a demo sample","text":"We select a single correctly classified sample to attack. The sample must be wrapped as a named tuple (data=x, label=y) — this is the format that AdversarialAttacks.jl expects.\n\ndemo_idx = 25 # number zero\n\nx_orig = X[:, :, :, demo_idx:demo_idx]\nlabel_onehot = y[:, demo_idx]\n\ntrue_label = argmax(label_onehot)                 # 1–10 index\ntrue_digit = Flux.onecold(label_onehot, 0:9)      # 0–9 digit\n\nsample = (data = x_orig, label = label_onehot)\n\n# Clean prediction\norig_pred = model(x_orig)\norig_true_prob = orig_pred[true_label]\n\nclean_label = argmax(orig_pred)[1]\nclean_digit = Flux.onecold(orig_pred, 0:9)[1]\n\nprintln(\"Chosen sample index: $demo_idx\")\nprintln(\"True digit: $true_digit  (index=$true_label)\")\nprintln(\"Clean prediction: $clean_digit  (index=$clean_label)\")\nprintln(\"Clean probs: \", round.(orig_pred, digits = 3))\nprintln(\"Clean true prob: \", round(orig_true_prob, digits = 3))","category":"section"},{"location":"tutorials/whitebox_fgsm_flux_mnist/#4.-Run-the-FGSM-white-box-attack","page":"White-Box – FGSM (Flux, MNIST)","title":"4. Run the FGSM white-box attack","text":"We construct an FGSM attack with a small perturbation budget ε. The attack() function computes the adversarial example by taking one gradient step in the direction that maximizes the loss.\n\nAfter the attack, we clamp pixel values back to [0, 1].\n\nε = 0.05f0\nfgsm_attack = FGSM(epsilon = ε)\nprintln(\"\\nRunning FGSM with ε = $ε ...\")\n\nx_adv = attack(fgsm_attack, model, sample)\nx_adv = clamp.(x_adv, 0.0f0, 1.0f0)  # keep pixels in [0,1]\n\nadv_pred = model(x_adv)\nadv_true_prob = adv_pred[true_label]\n\nadv_label = argmax(adv_pred)[1]\nadv_digit = Flux.onecold(adv_pred, 0:9)[1]\n\nprintln(\"\\nOriginal image stats   : min=$(minimum(x_orig)), max=$(maximum(x_orig))\")\nprintln(\"Adversarial image stats: min=$(minimum(x_adv)), max=$(maximum(x_adv))\")\nprintln(\"Perturbation L∞ norm   : \", maximum(abs.(x_adv .- x_orig)))","category":"section"},{"location":"tutorials/whitebox_fgsm_flux_mnist/#5.-Evaluate-the-attack","page":"White-Box – FGSM (Flux, MNIST)","title":"5. Evaluate the attack","text":"We check two success criteria:\n\nProbability drop: Did the true-class probability decrease?\nPrediction flip: Did the predicted label change from the correct one?\n\nprintln(\"\\nAdversarial probs: \", round.(adv_pred, digits = 3))\nprintln(\n    \"True prob: \", round(orig_true_prob, digits = 3), \" → \",\n    round(adv_true_prob, digits = 3)\n)\n\nprob_drop_success = adv_true_prob < orig_true_prob\nflip_success = (clean_label == true_label) && (adv_label != true_label)\n\nprintln(\n    \"[INFO] True-class prob drop success: \",\n    prob_drop_success, \"  (\",\n    round(orig_true_prob, digits = 3), \" → \",\n    round(adv_true_prob, digits = 3), \")\"\n)\n\nprintln(\n    \"[INFO] Prediction flip success: \",\n    flip_success, \"  (clean_digit=\", clean_digit,\n    \", adv_digit=\", adv_digit, \")\"\n)\n\nprintln(\"Digits summary: true=$true_digit, clean=$clean_digit, adv=$adv_digit\")","category":"section"},{"location":"tutorials/whitebox_fgsm_flux_mnist/#6.-Visualization","page":"White-Box – FGSM (Flux, MNIST)","title":"6. Visualization","text":"We plot three heatmaps side by side:\n\nOriginal: the clean MNIST image\nAdversarial: the perturbed image after the FGSM attack\nPerturbation: the pixel-wise difference, showing where the attack changed the image\n\np1 = heatmap(\n    reshape(x_orig[:, :, 1, 1], 28, 28),\n    title = \"Original (digit=$true_digit)\",\n    color = :grays, aspect_ratio = 1, size = (300, 300)\n)\n\np2 = heatmap(\n    reshape(x_adv[:, :, 1, 1], 28, 28),\n    title = \"Adversarial (digit=$adv_digit)\",\n    color = :grays, aspect_ratio = 1, size = (300, 300)\n)\n\np3 = heatmap(\n    reshape(x_adv[:, :, 1, 1] .- x_orig[:, :, 1, 1], 28, 28),\n    title = \"Perturbation (ε=$ε)\",\n    color = :RdBu, aspect_ratio = 1, size = (300, 300)\n)\n\nfig = plot(p1, p2, p3, layout = (1, 3), size = (900, 300))\nsavefig(fig, joinpath(@__DIR__, \"mnist_fgsm.svg\")) #hide\nfig #hide","category":"section"},{"location":"tutorials/whitebox_fgsm_flux_mnist/#Common-edits-to-try","page":"White-Box – FGSM (Flux, MNIST)","title":"Common edits to try","text":"Change ε (e.g., 0.05f0 → 0.1f0 or 0.01f0) to make perturbations stronger or weaker.\nChange demo_idx to attack different digits.\nIncrease training epochs or use more samples for a stronger base classifier.","category":"section"},{"location":"FGSM/#FGSM-(White-Box-Attack)","page":"FGSM (White-Box)","title":"FGSM (White-Box Attack)","text":"This page documents the Fast Gradient Sign Method (FGSM), a white-box adversarial attack that requires access to model gradients.","category":"section"},{"location":"FGSM/#FGSM-Implementation","page":"FGSM (White-Box)","title":"FGSM Implementation","text":"","category":"section"},{"location":"FGSM/#Quick-Example","page":"FGSM (White-Box)","title":"Quick Example","text":"using AdversarialAttacks\n\natk = FGSM(epsilon = 0.01f0)\nprintln(\"Attack: \", name(atk))\nprintln(\"Type check: \", atk isa WhiteBoxAttack)\nprintln(\"Epsilon: \", atk.epsilon)","category":"section"},{"location":"FGSM/#AdversarialAttacks.FGSM","page":"FGSM (White-Box)","title":"AdversarialAttacks.FGSM","text":"FGSM(; epsilon=0.1)\n\nA struct that can be used to create a white-box adversarial attack of type Fast Gradient Sign Method. Subtype of WhiteBoxAttack.\n\nArguments\n\nepsilon: Step size used to scale the sign of the gradient. Defaults to 0.1.\n\n\n\n\n\n","category":"type"},{"location":"FGSM/#AdversarialAttacks.attack-Tuple{FGSM, Flux.Chain, Any}","page":"FGSM (White-Box)","title":"AdversarialAttacks.attack","text":"attack(atk::FGSM, model, sample)\n\nPerform a Fast Gradient Sign Method (FGSM) white-box adversarial attack on the given model using the provided sample.\n\nArguments\n\natk::FGSM: An instance of the FGSM.\nmodel::FluxModel: The machine learning (deep learning) model to be attacked.\nsample: Input sample as a named tuple with data and label.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"BasicRandomSearch/#BasicRandomSearch-(Black-Box-Attack)","page":"BasicRandomSearch (Black-Box)","title":"BasicRandomSearch (Black-Box Attack)","text":"This page documents the BasicRandomSearch algorithm (SimBA variant), a black-box adversarial attack that only requires query access to model predictions.","category":"section"},{"location":"BasicRandomSearch/#BasicRandomSearch-Implementation","page":"BasicRandomSearch (Black-Box)","title":"BasicRandomSearch Implementation","text":"","category":"section"},{"location":"BasicRandomSearch/#Quick-Example","page":"BasicRandomSearch (Black-Box)","title":"Quick Example","text":"using AdversarialAttacks\n\natk = BasicRandomSearch(\n    epsilon = 0.3f0,\n    bounds = [(0.0f0, 1.0f0), (0.0f0, 1.0f0)],\n    max_iter = 50,\n)\nprintln(\"Attack: \", name(atk))\nprintln(\"Type check: \", atk isa BlackBoxAttack)\nprintln(\"Epsilon: \", atk.epsilon, \", Max iter: \", atk.max_iter)","category":"section"},{"location":"BasicRandomSearch/#AdversarialAttacks.BasicRandomSearch","page":"BasicRandomSearch (Black-Box)","title":"AdversarialAttacks.BasicRandomSearch","text":"BasicRandomSearch(; epsilon=0.1, max_iter=50, bounds=nothing, rng=Random.default_rng())\n\nSubtype of BlackBoxAttack. Creates adversarial examples using the SimBA random search algorithm.\n\nArguments\n\nepsilon: Step size for perturbations (default: 0.1)\nmax_iter: Maximum number of iterations for searching (default: 50).            Each iteration randomly selects a coordinate to perturb.\nbounds: Optional vector of (lower, upper) tuples specifying per-feature bounds.           If nothing, defaults to [0, 1] for all features (suitable for normalized images).           For tabular data, provide bounds matching feature ranges, e.g.,           [(4.3, 7.9), (2.0, 4.4), ...] for Iris-like data.\nrng: Random number generator for reproducibility (default: Random.default_rng())\n\n\n\n\n\n","category":"type"},{"location":"BasicRandomSearch/#AdversarialAttacks.attack-Tuple{BasicRandomSearch, Flux.Chain, Any}","page":"BasicRandomSearch (Black-Box)","title":"AdversarialAttacks.attack","text":"attack(atk::BasicRandomSearch, model::Chain, sample)\n\nPerform a black-box adversarial attack on the given model using the provided sample using the Basic Random Search variant SimBA.\n\nArguments\n\natk::BasicRandomSearch: An instance of the BasicRandomSearch (BlackBox) attack.\nmodel::Chain: The machine learning (deep learning, classical machine learning) model to be attacked.\nsample: The input sample to be changed.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"BasicRandomSearch/#AdversarialAttacks.attack-Tuple{BasicRandomSearch, DecisionTree.DecisionTreeClassifier, Any}","page":"BasicRandomSearch (Black-Box)","title":"AdversarialAttacks.attack","text":"attack(atk::BasicRandomSearch, model::DecisionTreeClassifier, sample)\n\nPerform a black-box adversarial attack on a DecisionTreeClassifier using BasicRandomSearch (SimBA).\n\nArguments\n\natk::BasicRandomSearch: Attack instance with epsilon and optional bounds.\nmodel::DecisionTreeClassifier: DecisionTree.jl classifier to attack.\nsample: NamedTuple with data and label fields.\n\nReturns\n\nAdversarial example (same shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"tutorials/blackbox_basicrandomsearch_decisiontree_iris/#Black-Box-Basic-Random-Search-Attack-on-Iris-(DecisionTree)","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"Black-Box Basic Random Search Attack on Iris (DecisionTree)","text":"This tutorial demonstrates how to perform a black-box adversarial attack using Basic Random Search (a SimBA-style algorithm) against a Decision Tree classifier trained on the Iris dataset.\n\nA black-box attack has no access to model gradients or internal parameters — it can only query the model for predictions. Basic Random Search is a greedy coordinate-cycling algorithm: it iterates over features in a random order and, for each feature, tries a small perturbation of +ε or −ε. If the perturbation reduces the true-class probability, it keeps the change; otherwise it reverts it. The search stops early when the predicted class flips.\n\nWhat you will learn:\n\nHow to train a DecisionTree classifier on Iris\nHow to construct a BasicRandomSearch attack with AdversarialAttacks.jl\nHow to evaluate whether the attack succeeded\nHow to visualize original vs adversarial samples in feature space","category":"section"},{"location":"tutorials/blackbox_basicrandomsearch_decisiontree_iris/#Prerequisites","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"Prerequisites","text":"Make sure you have the following packages installed: RDatasets, DecisionTree, Flux, OneHotArrays, Plots, and AdversarialAttacks.\n\nusing Random\nusing RDatasets\nusing DecisionTree\nusing Flux\nusing OneHotArrays: OneHotVector\nusing AdversarialAttacks\nusing Plots\n\nRandom.seed!(1234)","category":"section"},{"location":"tutorials/blackbox_basicrandomsearch_decisiontree_iris/#1.-Train-a-DecisionTree-on-Iris","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"1. Train a DecisionTree on Iris","text":"We load the classic Iris dataset, extract features and labels, and fit a DecisionTreeClassifier with a maximum depth of 3.\n\niris = dataset(\"datasets\", \"iris\")\nX = Matrix{Float64}(iris[:, 1:4])\ny_str = String.(iris.Species)\nclasses = [\"setosa\", \"versicolor\", \"virginica\"]\n\ndt_model = DecisionTreeClassifier(\n    max_depth = 3,\n    min_samples_leaf = 1,\n    min_samples_split = 2,\n    classes = classes,\n)\nfit!(dt_model, X, y_str)\n\nprintln(\"Trained DecisionTreeClassifier on Iris.\")\nprintln(\"Classes = \", dt_model.classes)\n\n# helper\nfunction predict_class_index(model::DecisionTreeClassifier, x::AbstractVector)\n    x_mat = reshape(Float64.(x), 1, :)\n    probs = vec(DecisionTree.predict_proba(model, x_mat))\n    return argmax(probs)\nend","category":"section"},{"location":"tutorials/blackbox_basicrandomsearch_decisiontree_iris/#2.-Pick-a-correctly-classified-demo-sample","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"2. Pick a correctly classified demo sample","text":"We search for a correctly classified versicolor sample to use as our attack target. Versicolor sits near the decision boundary with virginica, making it a good candidate for a successful perturbation.\n\ndemo_idx = findfirst(==(\"versicolor\"), y_str)\n\nfor i in 1:size(X, 1)\n    y_str[i] == \"versicolor\" || continue\n    xi = X[i, :]\n    true_idx_i = findfirst(==(y_str[i]), classes)\n    pred_idx_i = predict_class_index(dt_model, xi)\n    if pred_idx_i == true_idx_i\n        global demo_idx = i\n        break\n    end\nend\n\nx0 = X[demo_idx, :]\nlabel_str = y_str[demo_idx]\ntrue_idx = findfirst(==(label_str), classes)\n\nprintln(\"\\nChosen demo sample index: \", demo_idx)\nprintln(\"Feature vector: \", x0)\nprintln(\"True label string: \", label_str, \" (index \", true_idx, \")\")","category":"section"},{"location":"tutorials/blackbox_basicrandomsearch_decisiontree_iris/#3.-Build-the-sample-NamedTuple","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"3. Build the sample NamedTuple","text":"The attack interface expects a named tuple (data=..., label=...) where label is a one-hot encoded vector.\n\ny0 = Flux.onehot(true_idx, 1:length(classes))\nsample = (data = Float32.(x0), label = y0)\n\nx0_mat = reshape(Float64.(x0), 1, :)\norig_probs_vec = vec(DecisionTree.predict_proba(dt_model, x0_mat))\norig_true_prob = orig_probs_vec[true_idx]\n\nprintln(\"\\nOriginal probabilities: \", orig_probs_vec)\nprintln(\"Original predicted class index = \", argmax(orig_probs_vec))","category":"section"},{"location":"tutorials/blackbox_basicrandomsearch_decisiontree_iris/#4.-Run-BasicRandomSearch","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"4. Run BasicRandomSearch","text":"We configure the attack with:\n\nepsilon = 0.3: maximum perturbation per feature\nbounds: valid ranges for each Iris feature\nmax_iter = 100: number of random search iterations\n\nThe algorithm cycles through features in a random order and tries ±ε for each one. It greedily keeps any perturbation that reduces the true-class probability and stops early if the predicted class flips.\n\nε = 0.3f0\natk = BasicRandomSearch(\n    epsilon = ε,\n    bounds = [(4.3f0, 7.9f0), (2.0f0, 4.4f0), (1.0f0, 6.9f0), (0.1f0, 2.5f0)],\n    max_iter = 100,\n)\nprintln(\"\\nRunning BasicRandomSearch with epsilon = \", ε, \" and max_iter = \", atk.max_iter, \" ...\")\nRandom.seed!(42)\n\nx_adv = attack(atk, dt_model, sample)\n\nx_adv_mat = reshape(Float64.(x_adv), 1, :)\nadv_probs_vec = vec(DecisionTree.predict_proba(dt_model, x_adv_mat))\nadv_true_prob = adv_probs_vec[true_idx]\n\nprintln(\"\\nOriginal feature vector:     \", sample.data)\nprintln(\"Adversarial feature vector: \", x_adv)\n\nprintln(\"\\nOriginal probs:     \", orig_probs_vec)\nprintln(\"Adversarial probs: \", adv_probs_vec)","category":"section"},{"location":"tutorials/blackbox_basicrandomsearch_decisiontree_iris/#5.-Evaluate-the-attack","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"5. Evaluate the attack","text":"We check whether the attack decreased the true-class confidence.\n\nprintln(\"\\nTrue-class probability before attack: \", orig_true_prob)\nprintln(\"True-class probability after attack:  \", adv_true_prob)\n\nif adv_true_prob < orig_true_prob\n    println(\"\\n[INFO] Attack decreased the true-class confidence (success).\")\nelse\n    println(\"\\n[INFO] True-class confidence did not decrease.\")\nend","category":"section"},{"location":"tutorials/blackbox_basicrandomsearch_decisiontree_iris/#6.-Visualization","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"6. Visualization","text":"We create two scatter plots showing 2D projections of the Iris dataset (features 1 & 2, and features 3 & 4). The original sample is shown as a black star and the adversarial sample as an orange star, with an arrow indicating the perturbation direction.\n\nHow to read the plots: The background points (circles, triangles, squares) show the training data coloured by class. The black star marks the original sample's position and the orange star marks where the adversarial perturbation moved it. A gray arrow connects the two. If the attack succeeded, the orange star will sit in (or near) a region dominated by a different class, meaning the classifier's prediction flipped. If both stars overlap, the attack did not find a perturbation that changed the prediction.\n\nidx_setosa = findall(==(\"setosa\"), y_str)\nidx_versicolor = findall(==(\"versicolor\"), y_str)\nidx_virginica = findall(==(\"virginica\"), y_str)\n\norig_pred_class = classes[argmax(orig_probs_vec)]\nadv_pred_class = classes[argmax(adv_probs_vec)]\n\n# Plot 1: features 1 & 2\np12 = plot(\n    xlabel = \"SepalLength\",\n    ylabel = \"SepalWidth\",\n    title = \"Iris (features 1&2)\",\n)\nscatter!(\n    p12, X[idx_setosa, 1], X[idx_setosa, 2],\n    color = :blue, markershape = :circle, alpha = 0.6, label = \"setosa\",\n)\nscatter!(\n    p12, X[idx_versicolor, 1], X[idx_versicolor, 2],\n    color = :green, markershape = :utriangle, alpha = 0.6, label = \"versicolor\",\n)\nscatter!(\n    p12, X[idx_virginica, 1], X[idx_virginica, 2],\n    color = :red, markershape = :square, alpha = 0.6, label = \"virginica\",\n)\nscatter!(\n    p12, [x0[1]], [x0[2]],\n    markersize = 12, color = :black, markershape = :star5,\n    markerstrokecolor = :white, label = \"original ($orig_pred_class)\",\n)\nscatter!(\n    p12, [x_adv[1]], [x_adv[2]],\n    markersize = 12, color = :orange, markershape = :star5,\n    markerstrokecolor = :white, label = \"adversarial ($adv_pred_class)\",\n)\nplot!(\n    p12, [x0[1], x_adv[1]], [x0[2], x_adv[2]],\n    arrow = true, color = :gray, linewidth = 1.5, label = \"\",\n)\n\n# Plot 2: features 3 & 4\np34 = plot(xlabel = \"PetalLength\", ylabel = \"PetalWidth\", title = \"Iris (features 3&4)\")\nscatter!(\n    p34, X[idx_setosa, 3], X[idx_setosa, 4],\n    color = :blue, markershape = :circle, alpha = 0.6, label = \"setosa\",\n)\nscatter!(\n    p34, X[idx_versicolor, 3], X[idx_versicolor, 4],\n    color = :green, markershape = :utriangle, alpha = 0.6, label = \"versicolor\",\n)\nscatter!(\n    p34, X[idx_virginica, 3], X[idx_virginica, 4],\n    color = :red, markershape = :square, alpha = 0.6, label = \"virginica\",\n)\nscatter!(\n    p34, [x0[3]], [x0[4]],\n    markersize = 12, color = :black, markershape = :star5,\n    markerstrokecolor = :white, label = \"original ($orig_pred_class)\",\n)\nscatter!(\n    p34, [x_adv[3]], [x_adv[4]],\n    markersize = 12, color = :orange, markershape = :star5,\n    markerstrokecolor = :white, label = \"adversarial ($adv_pred_class)\",\n)\nplot!(\n    p34, [x0[3], x_adv[3]], [x0[4], x_adv[4]],\n    arrow = true, color = :gray, linewidth = 1.5, label = \"\",\n)\n\nfig = plot(p12, p34, layout = (1, 2), size = (1000, 400), margin = 5Plots.mm)\nsavefig(fig, joinpath(@__DIR__, \"iris_bsr.svg\")) #hide\nfig #hide","category":"section"},{"location":"tutorials/blackbox_basicrandomsearch_decisiontree_iris/#Common-edits-to-try","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"Common edits to try","text":"Increase epsilon to make perturbations stronger.\nIncrease max_iter to give the attack more search time.\nAdjust bounds to constrain or widen the search domain.\nAttack other samples by changing how demo_idx is chosen.\nTarget a specific class by modifying the attack's objective function.\nChange Random.seed! values to explore different search trajectories.","category":"section"},{"location":"examples/#Tutorials-and-Examples-Overview","page":"Overview","title":"Tutorials & Examples Overview","text":"The Tutorials & Examples section provides end‑to‑end, runnable scripts that show how to use AdversarialAttacks.jl in realistic workflows. Each tutorial is generated from the scripts in examples/ using Literate.jl, so the code runs during the documentation build and all output is captured inline.","category":"section"},{"location":"examples/#The-current-tutorials-cover:","page":"Overview","title":"The current tutorials cover:","text":"White-box FGSM attack against a small Flux CNN on MNIST — see the tutorial.\nBlack-box Basic Random Search (SimBA-style) attack against a DecisionTree classifier on the Iris dataset — see the tutorial.","category":"section"},{"location":"examples/#Running-locally","page":"Overview","title":"Running locally","text":"The tutorial scripts in examples/ are fully standalone. You can run them directly from the repository root:\n\njulia> using Pkg\njulia> Pkg.activate(\"./examples\")   # activate the examples environment from the repository root\njulia> include(\"examples/whitebox_fgsm_flux_mnist.jl\")\njulia> include(\"examples/blackbox_basicrandomsearch_decisiontree_iris.jl\")\n\nOr run them directly from the examples/ directory using the project flag:\n\njulia --project=. -e 'using Pkg; Pkg.instantiate(); Pkg.precompile()'\njulia --project=. whitebox_fgsm_flux_mnist.jl\njulia --project=. blackbox_basicrandomsearch_decisiontree_iris.jl","category":"section"},{"location":"examples/#Notes-and-tips","page":"Overview","title":"Notes and tips","text":"Each example uses a small training setup so it runs quickly for demonstration purposes. For a more applicable example, re-run with more epochs and save the trained model.\nScripts will print training and attack statistics and generate plot files.\nIf you want to reproduce results exactly, set seeds as shown in the examples.","category":"section"},{"location":"examples/#Next-steps","page":"Overview","title":"Next steps","text":"Follow the individual tutorial pages for step‑by‑step code with executed output and plots","category":"section"},{"location":"#AdversarialAttacks.jl","page":"Getting Started","title":"AdversarialAttacks.jl","text":"(Image: Dev) (Image: Build Status) (Image: Coverage) (Image: License)\n\nAdversarialAttacks.jl is a lightweight Julia package for experimenting with adversarial attacks against neural networks and tree‑based models, focusing on FGSM (white‑box) and random‑search–based (black‑box) attacks.\n\nCurrently, this package supports only models implemented as Flux chains (neural networks) and decision trees from the DecisionTree.jl package. Support for other model types may be added in the future.","category":"section"},{"location":"#Installation","page":"Getting Started","title":"Installation","text":"You can install the package via the Julia package manager. In the Julia REPL, run:\n\njulia> ]add https://github.com/shnaky/AdversarialAttacks.jl","category":"section"},{"location":"#Examples","page":"Getting Started","title":"Examples","text":"For comprehensive, executable tutorials with detailed explanations and visualizations, see the Tutorials & Examples section in the documentation.\n\nThe following example shows how to create an adversarial sample from a single input sample using the FGSM attack:","category":"section"},{"location":"#FGSM-attack-Flux-Integration-(white-box)","page":"Getting Started","title":"FGSM attack - Flux Integration (white-box)","text":"julia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(\n           Dense(2, 2, tanh),\n           Dense(2, 2),\n           softmax,\n       )\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> sample = (data=rand(Float32, 2, 1), label=Flux.onehot(1, 1:2) )\n\njulia> adv_sample = attack(fgsm, model, sample)","category":"section"},{"location":"#BasicRandomSearch-attack-DecisionTree-integration-(black-box)","page":"Getting Started","title":"BasicRandomSearch attack - DecisionTree integration (black-box)","text":"julia> using AdversarialAttacks\n\njulia> using DecisionTree\n\njulia> classes = [1, 2, 3]\n\njulia> X = rand(24, 4) .* 4\n\njulia> y = vcat(\n    fill(classes[1], 8),\n    fill(classes[2], 8),\n    fill(classes[3], 8),\n)\n\njulia> tree = DecisionTreeClassifier(; classes = classes)\n\njulia> fit!(tree, X, y)\n\njulia> sample = (data = X[:, 1], label = y[1])\n\njulia> atk = BasicRandomSearch(epsilon = 0.1f0, max_iter = 50)\n\njulia> x_adv = attack(atk, tree, sample)","category":"section"},{"location":"#Batch-Example","page":"Getting Started","title":"Batch Example","text":"You can also apply an attack to a batch of samples represented as a tensor.\n\njulia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(Dense(4, 8, relu), Dense(8, 2), softmax)\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> X = rand(Float32, 4, 3)  # 3 samples, 4 features each\n\njulia> Y = Flux.onehotbatch([1, 2, 1], 1:2)  # labels for each sample\n\njulia> tensor = (data=X, label=Y)\n\njulia> adv_samples = attack(fgsm, model, tensor)","category":"section"},{"location":"#Evaluation-Example","page":"Getting Started","title":"Evaluation Example","text":"Get an evaluation report on your adversarial attack.\n\njulia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(Dense(4, 3), softmax)\n\njulia> test_data = [ (data=randn(Float32, 4), label=Flux.onehot(rand(1:3), 1:3)) for _ in 1:10 ]\n\njulia> fgsm = FGSM(epsilon=0.5)\n\njulia> report = evaluate_robustness(model, fgsm, test_data)\n\njulia> println(report)\n=== Robustness Evaluation Report ===\n\nDataset\n  Total samples evaluated        : 10\n  Clean-correct samples          : 4 / 10\n\nClean Performance\n  Clean accuracy                 : 40.0%\n\nAdversarial Performance\n  Adversarial accuracy           : 0.0%\n\nAttack Effectiveness\n  Successful attacks             : 4 / 4\n  Attack success rate (ASR)      : 100.0%\n  Robustness score (1 - ASR)     : 0.0%\n\nPerturbation Analysis (L_inf norm)\n  Maximum perturbation           : 0.5\n  Mean perturbation              : 0.5\n\nNotes\n  • Attack success is counted only when:\n    - the clean prediction is correct\n    - the adversarial prediction is incorrect\n===================================","category":"section"},{"location":"#License","page":"Getting Started","title":"License","text":"This package is licensed under the MIT License. See LICENSE for details.","category":"section"},{"location":"#AI-Assistance","page":"Getting Started","title":"AI Assistance","text":"This project uses AI coding assistants for routine maintenance tasks such as:\n\nCode style fixes and documentation improvements\nConsolidating imports and minor refactoring\nAddressing code review feedback\n\nAll AI-generated changes are reviewed before merging.","category":"section"},{"location":"evaluation/#Robustness-Evaluation-Suite","page":"Robustness Evaluation Suite","title":"Robustness Evaluation Suite","text":"This page documents Robustness Evaluation Suite.\n\nusing AdversarialAttacks\nprintln(\"RobustnessReport fields: \", fieldnames(RobustnessReport))","category":"section"},{"location":"evaluation/#AdversarialAttacks.RobustnessReport","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.RobustnessReport","text":"RobustnessReport\n\nReport on model robustness against an adversarial attack. Printing a RobustnessReport (via println(report)) displays a nicely formatted summary including clean/adversarial accuracy, attack success rate, and robustness score.\n\nFields\n\nnum_samples::Int: Total samples evaluated\nnum_clean_correct::Int: Samples correctly classified before attack\nclean_accuracy::Float64: Accuracy on clean samples\nadv_accuracy::Float64: Accuracy on adversarial samples\nattack_success_rate::Float64: (ASR) Fraction of successful attacks (on correctly classified samples)\nrobustness_score::Float64: 1.0 - attacksuccessrate (ASR)\nnum_successful_attacks::Int: Number of successful attacks\nlinf_norm_max::Float64: Maximum L∞ norm of perturbations across all samples\nlinf_norm_mean::Float64: Mean L∞ norm of perturbations across all samples\nl2_norm_max::Float64: Maximum L2 norm of perturbations across all samples\nl2_norm_mean::Float64: Mean L2 norm of perturbations across all samples\nl1_norm_max::Float64: Maximum L1 norm of perturbations across all samples\nl1_norm_mean::Float64: Mean L1 norm of perturbations across all samples\n\nNote\n\nAn attack succeeds when the clean prediction is correct but the adversarial prediction is incorrect.\n\nThe L∞ norm measures the maximum absolute change in any feature of the input.\nThe L2 norm measures the Euclidean distance between original and adversarial samples.\nThe L1 norm measures the Manhattan distance (sum of absolute differences).\n\n\n\n\n\n","category":"type"},{"location":"evaluation/#AdversarialAttacks.calculate_metrics-NTuple{5, Any}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.calculate_metrics","text":"calculate_metrics(n_test, num_clean_correct, num_adv_correct,\n                  num_successful_attacks, l_norms)\n\nCompute accuracy, attack success, robustness, and perturbation norm statistics for adversarial evaluation.\n\nArguments\n\nn_test: Number of test samples.\nnum_clean_correct: Number of correctly classified clean samples.\nnum_adv_correct: Number of correctly classified adversarial samples.\nnum_successful_attacks: Number of successful adversarial attacks.\nl_norms: Dictionary containing perturbation norm arrays with keys :linf, :l2, and :l1.\n\nReturns\n\nA RobustnessReport containing accuracy, robustness, and norm summary metrics (maximum and mean) for all three norm types.\n\n\n\n\n\n","category":"method"},{"location":"evaluation/#AdversarialAttacks.compute_norm-Tuple{Any, Any, Real}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.compute_norm","text":"compute_norm(sample_data, adv_data, p::Real)\n\nCompute the Lp norm of the perturbation between original data and adversarial data.\n\nThis function uses LinearAlgebra.norm for optimal performance and numerical stability.\n\nArguments\n\nsample_data: Original sample data.\nadv_data: Adversarially perturbed version of sample_data.\np::Real: Order of the norm. Must be positive or Inf.\nCommon values: 1 (Manhattan/L1), 2 (Euclidean/L2), Inf (maximum/L∞).\n\nReturns\n\nFloat64: The Lp norm of the perturbation ||adv_data - sample_data||_p.\n\nExamples\n\noriginal = [1.0, 2.0, 3.0]\nadversarial = [1.5, 2.5, 3.5]\n\ncompute_norm(original, adversarial, 2)    # L2 (Euclidean) norm\ncompute_norm(original, adversarial, 1)    # L1 (Manhattan) norm\ncompute_norm(original, adversarial, Inf)  # L∞ (maximum) norm\n\nReferences\n\nLp space: https://en.wikipedia.org/wiki/Lp_space\n\n\n\n\n\n","category":"method"},{"location":"evaluation/#AdversarialAttacks.evaluate_robustness-Tuple{Any, Any, Any}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.evaluate_robustness","text":"evaluate_robustness(model, atk, test_data; num_samples=100)\n\nEvaluate model robustness by running attack on multiple samples.\n\nFor each sample, computes clean and adversarial predictions, tracks attack success, and calculates perturbation norms (L∞, L2, and L1).\n\nArguments\n\nmodel: The model to evaluate.\natk: The attack to use.\ntest_data: Collection of test samples.\nnum_samples::Int=100: Number of samples to test. If more than available samples, uses all available samples.\n\nReturns\n\nRobustnessReport: Report containing accuracy, attack success rate, robustness metrics,\n\nand perturbation statistics for L∞, L2, and L1 norms.\n\nExample\n\nreport = evaluate_robustness(model, FGSM(epsilon=0.1), test_data, num_samples=50)\nprintln(report)\n\n\n\n\n\n","category":"method"},{"location":"evaluation/#AdversarialAttacks.evaluation_curve-Tuple{Any, Type{<:AbstractAttack}, Vector{Float64}, Any}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.evaluation_curve","text":"evaluation_curve(model, atk_type, epsilons, test_data; num_samples=100)\n\nEvaluate model robustness across a range of attack strengths.\n\nFor each value in epsilons, an attack of type atk_type is instantiated and used to compute clean accuracy, adversarial accuracy, attack success rate, robustness score, and perturbation norms (L∞, L2, and L1).\n\nArguments\n\nmodel: Model to be evaluated.\natk_type: Adversarial attack type.\nepsilons: Vector of attack strengths.\ntest_data: Test dataset.\n\nKeyword Arguments\n\nnum_samples::Int=100: Number of samples used for each epsilon evaluation.\n\nReturns\n\nA dictionary containing evaluation metrics for each epsilon value:\n:epsilons: Attack strength values\n:clean_accuracy: Clean accuracy for each epsilon\n:adv_accuracy: Adversarial accuracy for each epsilon\n:attack_success_rate: Attack success rate for each epsilon\n:robustness_score: Robustness score (1 - ASR) for each epsilon\n:linf_norm_mean, :linf_norm_max: L∞ norm statistics\n:l2_norm_mean, :l2_norm_max: L2 norm statistics\n:l1_norm_mean, :l1_norm_max: L1 norm statistics\n\nExample\n\nresults = evaluation_curve(model, FGSM, [0.01, 0.05, 0.1], test_data, num_samples=100)\nprintln(\"Attack success rates: \", results[:attack_success_rate])\n\n\n\n\n\n","category":"method"}]
}
