var documenterSearchIndex = {"docs":
[{"location":"attack_interface/#Attack-Interface","page":"Attack Interface","title":"Attack Interface","text":"This page documents the shared attack abstractions.","category":"section"},{"location":"attack_interface/#AdversarialAttacks.Attack.AbstractAttack","page":"Attack Interface","title":"AdversarialAttacks.Attack.AbstractAttack","text":"Abstract supertype for all adversarial attacks.\n\nExpected interface (to be implemented per concrete attack):\n\nname(::AbstractAttack)::String\nhyperparameters(::AbstractAttack)::Dict{String,Any}\ncraft(sample, model, atk::AbstractAttack; kwargs...) returning an adversarial example\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.Attack.BlackBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.Attack.BlackBoxAttack","text":"Abstract type for black-box adversarial attacks.\n\nBlack-box attacks only have access to the model's input-output behavior, without knowledge of the model's internals, gradients, or architecture. These attacks typically rely on query-based methods (e.g., optimization via repeated queries) or transferability from surrogate models.\n\nUse BlackBoxAttack when you do not have access to the model's internal parameters or gradients, such as in deployed systems or APIs.  In contrast, use WhiteBoxAttack when you have full access to the model's internals and can leverage gradient information for crafting adversarial examples.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.Attack.WhiteBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.Attack.WhiteBoxAttack","text":"Abstract type for white-box adversarial attacks.\n\nWhite-box attacks have full access to the model's internals, including gradients, weights, and architecture. This enables the use of gradient-based optimization and other techniques to craft adversarial examples.\n\nUse this type when the attacker can inspect and manipulate the model's internal parameters and computations. If only input-output access is available, use BlackBoxAttack instead.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.Attack.craft-Tuple{Any, Any, AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.Attack.craft","text":"craft(sample, model, attack::AbstractAttack; kwargs...) -> adversarial_sample\n\nCraft an adversarial example by applying the attack to a sample.\n\nArguments\n\nsample: Input sample to perturb (e.g., image, text)\nmodel: Target model to attack\nattack::AbstractAttack: Attack configuration and algorithm\nkwargs...: Additional attack-specific parameters\n\nReturns\n\nAdversarial example with the same shape as the input sample\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#AdversarialAttacks.Attack.hyperparameters-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.Attack.hyperparameters","text":"Return hyperparameters for an attack\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#AdversarialAttacks.Attack.name-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.Attack.name","text":"Human-readable name for an attack\n\n\n\n\n\n","category":"method"},{"location":"model_interface/#Model-Interface","page":"Model Interface","title":"Model Interface","text":"","category":"section"},{"location":"model_interface/#AdversarialAttacks.Model.AbstractModel","page":"Model Interface","title":"AdversarialAttacks.Model.AbstractModel","text":"Abstract base for all models that can be attacked.\n\nMust implement: predict(model, x), loss(model, x, y). Optional: params(model) for white-box attacks.\n\n\n\n\n\n","category":"type"},{"location":"model_interface/#AdversarialAttacks.Model.DifferentiableModel","page":"Model Interface","title":"AdversarialAttacks.Model.DifferentiableModel","text":"Models that support gradient-based white-box attacks (e.g. neural networks with Flux.jl).\n\n\n\n\n\n","category":"type"},{"location":"model_interface/#AdversarialAttacks.Model.NonDifferentiableModel","page":"Model Interface","title":"AdversarialAttacks.Model.NonDifferentiableModel","text":"Models without gradient access for black-box attacks (e.g. traditional ML models such as decision tree, SVM, logistic regression).\n\n\n\n\n\n","category":"type"},{"location":"model_interface/#AdversarialAttacks.Model.loss-Tuple{AbstractModel, Any, Any}","page":"Model Interface","title":"AdversarialAttacks.Model.loss","text":"loss(m::AbstractModel, x, y) -> Real\n\nCompute a scalar loss for input–target pair (x, y).\n\nWhite-box attacks will typically differentiate this loss with respect to the input x.\n\nArguments\n\nm::AbstractModel: Model instance.\nx: Input data.\ny: Target labels (e.g. one-hot vectors or class indices).\n\nReturns\n\nReal: Scalar loss value used for training or attacks.\n\nExamples\n\nℓ = loss(model, xbatch, ybatch)\n\n\n\n\n\n","category":"method"},{"location":"model_interface/#AdversarialAttacks.Model.name-Tuple{AbstractModel}","page":"Model Interface","title":"AdversarialAttacks.Model.name","text":"name(m::AbstractModel) -> String\n\nReturn a human-readable name for the model.\n\nArguments\n\nm::AbstractModel: Model instance.\n\nReturns\n\nString: Descriptive name of the model type (e.g. \"FluxModel\", \"TreeModel\").\n\nExamples\n\nname(FluxModel(chain)) # \"FluxModel\"\n\n\n\n\n\n","category":"method"},{"location":"model_interface/#AdversarialAttacks.Model.params-Tuple{AbstractModel}","page":"Model Interface","title":"AdversarialAttacks.Model.params","text":"params(m::AbstractModel)\n\nReturn the trainable parameters of m, if available.\n\nWhite-box attacks may use this; black-box models can ignore it.\n\nArguments\n\nm::AbstractModel: Model instance.\n\nReturns\n\nImplementation-defined: Typically a collection of arrays or a parameter container (e.g. Flux.Params for FluxModel).\n\nExamples\n\nθ = params(model)\n\n\n\n\n\n","category":"method"},{"location":"model_interface/#AdversarialAttacks.Model.predict-Tuple{AbstractModel, Any}","page":"Model Interface","title":"AdversarialAttacks.Model.predict","text":"predict(m::AbstractModel, x) -> y\n\nCompute model predictions for input x.\n\nFor classifiers, this usually returns logits or class probabilities.\n\nArguments\n\nm::AbstractModel: Model instance.\nx: Input data (e.g. feature vector, image batch).\n\nReturns\n\ny: Model output with a type and shape defined by the concrete model, usually matching the shape expected by loss.\n\nExamples\n\nŷ = predict(model, x_batch)\n\n\n\n\n\n","category":"method"},{"location":"#AdversarialAttacks","page":"Home","title":"AdversarialAttacks","text":"Documentation for AdversarialAttacks.\n\n","category":"section"}]
}
