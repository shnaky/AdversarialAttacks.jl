var documenterSearchIndex = {"docs":
[{"location":"examples/whitebox_fgsm_flux_mnist/#White-Box-–-FGSM-(Flux,-MNIST)","page":"White-Box – FGSM (Flux, MNIST)","title":"White-Box – FGSM (Flux, MNIST)","text":"This tutorial illustrates a white‑box attack using the Fast Gradient Sign Method (FGSM) against a convolutional neural network trained on MNIST handwritten digits. The goal is to show how small, gradient‑based perturbations can flip the model’s prediction while remaining almost imperceptible.","category":"section"},{"location":"examples/whitebox_fgsm_flux_mnist/#What-the-script-does","page":"White-Box – FGSM (Flux, MNIST)","title":"What the script does","text":"Loads a subset of MNIST, normalizes pixel values to 0 1, and trains a simple CNN with Flux.\nWraps the trained a CNN model in Flux and crafts an adversarial example for a correctly classified digit using FGSM with a small L_infty budget.\nReports clean vs adversarial probabilities, the drop in true‑class confidence, and whether the predicted digit changes.","category":"section"},{"location":"examples/whitebox_fgsm_flux_mnist/#Visual-summary","page":"White-Box – FGSM (Flux, MNIST)","title":"Visual summary","text":"The effect of the attack is summarized by the following figure, included in the documentation as:\n\n(Image: FGSM attack on MNIST)\n\nThe three panels show:\n\nOriginal (digit = 0): The clean MNIST image that the model classifies correctly.\nAdversarial (digit = 7): The perturbed image after the FGSM attack, which the model now misclassifies as a different digit.\nPerturbation (epsilon = 00015): A heatmap of the pixel‑wise difference, highlighting that only a thin band of pixels around the digit is modified while the overall shape remains visually similar.\n\nThis figure makes the white‑box threat model and the subtlety of the perturbation immediately apparent.","category":"section"},{"location":"attack_interface/#Attack-Interface","page":"Attack Interface","title":"Attack Interface","text":"This page documents the shared attack abstractions.","category":"section"},{"location":"attack_interface/#AdversarialAttacks.AbstractAttack","page":"Attack Interface","title":"AdversarialAttacks.AbstractAttack","text":"Abstract supertype for all adversarial attacks.\n\nExpected interface (to be implemented per concrete attack):\n\nname(::AbstractAttack)::String\nattack(atk::AbstractAttack, model, sample; kwargs...) returning an adversarial example\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.WhiteBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.WhiteBoxAttack","text":"Abstract type for white-box adversarial attacks.\n\nWhite-box attacks have full access to the model's internals, including gradients, weights, and architecture. This enables the use of gradient-based optimization and other techniques to craft adversarial examples.\n\nUse this type when the attacker can inspect and manipulate the model's internal parameters and computations. If only input-output access is available, use BlackBoxAttack instead.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.BlackBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.BlackBoxAttack","text":"Abstract type for black-box adversarial attacks.\n\nBlack-box attacks only have access to the model's input-output behavior, without knowledge of the model's internals, gradients, or architecture. These attacks typically rely on query-based methods (e.g., optimization via repeated queries) or transferability from surrogate models.\n\nUse BlackBoxAttack when you do not have access to the model's internal parameters or gradients, such as in deployed systems or APIs.  In contrast, use WhiteBoxAttack when you have full access to the model's internals and can leverage gradient information for crafting adversarial examples.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.name-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.name","text":"name(atk::AbstractAttack) -> String\n\nHuman-readable name for an attack.\n\nReturns\n\nString: String representation of the attack type.\n\n\n\n\n\n","category":"method"},{"location":"interface/#Interface","page":"Interface","title":"Interface","text":"This page documents the Interface for user interaction.","category":"section"},{"location":"interface/#AdversarialAttacks.attack","page":"Interface","title":"AdversarialAttacks.attack","text":"attack(atk::AbstractAttack, model, sample; kwargs...) -> adversarial_sample\n\nGenerate an adversarial example by applying the attack to a sample.\n\nArguments\n\natk::AbstractAttack: Attack configuration and algorithm\nmodel: Target model to attack\nsample: Input sample to perturb (e.g., image, text)\nkwargs...: Additional attack-specific parameters\n\nReturns\n\nAdversarial example with the same shape as the input sample\n\n\n\n\n\nattack(atk::BasicRandomSearch, model::Chain, sample)\n\nPerform a black-box adversarial attack on the given model using the provided sample using the Basic Random Search variant SimBA.\n\nArguments\n\natk::BasicRandomSearch: An instance of the BasicRandomSearch (BlackBox) attack.\nmodel::Chain: The machine learning (deep learning, classical machine learning) model to be attacked.\nsample: The input sample to be changed.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\nattack(atk::BasicRandomSearch, model::DecisionTreeClassifier, sample)\n\nPerform a black-box adversarial attack on a DecisionTreeClassifier using BasicRandomSearch (SimBA).\n\nArguments\n\natk::BasicRandomSearch: Attack instance with epsilon and optional bounds.\nmodel::DecisionTreeClassifier: DecisionTree.jl classifier to attack.\nsample: NamedTuple with data and label fields.\n\nReturns\n\nAdversarial example (same shape as sample.data).\n\n\n\n\n\nattack(atk::BasicRandomSearch, mach::Machine, sample)\n\nBlack-box adversarial attack on an MLJ Machine (e.g. a RandomForestClassifier) using BasicRandomSearch (SimBA), via blackbox_predict/predict.\n\natk::BasicRandomSearch: Attack instance with epsilon and max_iter.\nmach::Machine: Trained MLJ machine with probabilistic predictions.\nsample: NamedTuple with data (feature vector) and label (true class index, 1-based).\n\nReturns an adversarial example with the same shape as sample.data.\n\n\n\n\n\nattack(atk::FGSM, model, sample)\n\nPerform a Fast Gradient Sign Method (FGSM) white-box adversarial attack on the given model using the provided sample.\n\nArguments\n\natk::FGSM: An instance of the FGSM.\nmodel::FluxModel: The machine learning (deep learning) model to be attacked.\nsample: Input sample as a named tuple with data and label.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"function"},{"location":"interface/#AdversarialAttacks.benchmark","page":"Interface","title":"AdversarialAttacks.benchmark","text":"benchmark(atk::AbstractAttack, model, dataset, metric::Function; kwargs...)\n\nEvaluate attack performance on a dataset with labels using a given metric.\n\nArguments\n\natk::AbstractAttack: Attack algorithm\nmodel: Target model to attack\ndataset: Dataset with samples and labels\nmetric::Function: Evaluation metric with signature metric(model, adv_samples, labels)\n\nReturns\n\nScalar metric value representing attack performance\n\n\n\n\n\n","category":"function"},{"location":"examples/blackbox_basicrandomsearch_decisiontree_iris/#Black-Box-–-Basic-Random-Search-(DecisionTree,-Iris)","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"Black-Box – Basic Random Search (DecisionTree, Iris)","text":"This tutorial demonstrates a black‑box adversarial attack using Basic Random Search on a decision tree trained on the Iris dataset. Unlike the MNIST example, the attack only relies on model queries and does not use gradients or internal parameters.","category":"section"},{"location":"examples/blackbox_basicrandomsearch_decisiontree_iris/#What-the-script-does","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"What the script does","text":"Trains a DecisionTreeClassifier on the four Iris features (sepal length/width, petal length/width) with a limited maximum depth.\nPicks a correctly classified sample and constructs a (data, label) pair compatible with the craft API.\nRuns a BasicRandomSearch attack with feature‑wise bounds, searching for a nearby point that reduces the true‑class probability and potentially changes the predicted class.","category":"section"},{"location":"examples/blackbox_basicrandomsearch_decisiontree_iris/#Visual-summary","page":"Black-Box – Basic Random Search (DecisionTree, Iris)","title":"Visual summary","text":"The resulting adversarial effect in feature space is summarized with:\n\n(Image: Basic Random Search on Iris)\n\nThe figure consists of two scatter plots:\n\nLeft (features 1 & 2): Sepal length vs sepal width, showing all Iris samples with different markers for setosa, versicolor, and virginica. In this projection the original and adversarial points almost overlap, because the attack changes features 1 and 2 only minimally.\nRight (features 3 & 4): Petal length vs petal width, again with class‑specific markers, and two highlighted points for the original sample and its adversarial counterpart.\n\nThe annotation in the right panel indicates the true class, the original prediction, and the adversarial prediction (for example, true: versicolor, original: versicolor, adversarial: virginica), making it clear how a small move in the feature space can cross the decision boundary of the tree in a purely query‑based setting.","category":"section"},{"location":"FGSM/#FGSM-(White-Box-Attack)","page":"FGSM (White-Box)","title":"FGSM (White-Box Attack)","text":"This page documents the Fast Gradient Sign Method (FGSM), a white-box adversarial attack that requires access to model gradients.","category":"section"},{"location":"FGSM/#FGSM-Implementation","page":"FGSM (White-Box)","title":"FGSM Implementation","text":"","category":"section"},{"location":"FGSM/#AdversarialAttacks.FGSM","page":"FGSM (White-Box)","title":"AdversarialAttacks.FGSM","text":"FGSM(; epsilon=0.1)\n\nA struct that can be used to create a white-box adversarial attack of type Fast Gradient Sign Method. Subtype of WhiteBoxAttack.\n\nArguments\n\nepsilon: Step size used to scale the sign of the gradient. Defaults to 0.1.\n\n\n\n\n\n","category":"type"},{"location":"FGSM/#AdversarialAttacks.attack-Tuple{FGSM, Flux.Chain, Any}","page":"FGSM (White-Box)","title":"AdversarialAttacks.attack","text":"attack(atk::FGSM, model, sample)\n\nPerform a Fast Gradient Sign Method (FGSM) white-box adversarial attack on the given model using the provided sample.\n\nArguments\n\natk::FGSM: An instance of the FGSM.\nmodel::FluxModel: The machine learning (deep learning) model to be attacked.\nsample: Input sample as a named tuple with data and label.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"BasicRandomSearch/#BasicRandomSearch-(Black-Box-Attack)","page":"BasicRandomSearch (Black-Box)","title":"BasicRandomSearch (Black-Box Attack)","text":"This page documents the BasicRandomSearch algorithm (SimBA variant), a black-box adversarial attack that only requires query access to model predictions.","category":"section"},{"location":"BasicRandomSearch/#BasicRandomSearch-Implementation","page":"BasicRandomSearch (Black-Box)","title":"BasicRandomSearch Implementation","text":"","category":"section"},{"location":"BasicRandomSearch/#AdversarialAttacks.BasicRandomSearch","page":"BasicRandomSearch (Black-Box)","title":"AdversarialAttacks.BasicRandomSearch","text":"BasicRandomSearch(; epsilon=0.1, max_iter=50, bounds=nothing, rng=Random.default_rng())\n\nSubtype of BlackBoxAttack. Creates adversarial examples using the SimBA random search algorithm.\n\nArguments\n\nepsilon: Step size for perturbations (default: 0.1)\nmax_iter: Maximum number of iterations for searching (default: 50).            Each iteration randomly selects a coordinate to perturb.\nbounds: Optional vector of (lower, upper) tuples specifying per-feature bounds.           If nothing, defaults to [0, 1] for all features (suitable for normalized images).           For tabular data, provide bounds matching feature ranges, e.g.,           [(4.3, 7.9), (2.0, 4.4), ...] for Iris-like data.\nrng: Random number generator for reproducibility (default: Random.default_rng())\n\n\n\n\n\n","category":"type"},{"location":"BasicRandomSearch/#AdversarialAttacks.attack-Tuple{BasicRandomSearch, Flux.Chain, Any}","page":"BasicRandomSearch (Black-Box)","title":"AdversarialAttacks.attack","text":"attack(atk::BasicRandomSearch, model::Chain, sample)\n\nPerform a black-box adversarial attack on the given model using the provided sample using the Basic Random Search variant SimBA.\n\nArguments\n\natk::BasicRandomSearch: An instance of the BasicRandomSearch (BlackBox) attack.\nmodel::Chain: The machine learning (deep learning, classical machine learning) model to be attacked.\nsample: The input sample to be changed.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"BasicRandomSearch/#AdversarialAttacks.attack-Tuple{BasicRandomSearch, DecisionTree.DecisionTreeClassifier, Any}","page":"BasicRandomSearch (Black-Box)","title":"AdversarialAttacks.attack","text":"attack(atk::BasicRandomSearch, model::DecisionTreeClassifier, sample)\n\nPerform a black-box adversarial attack on a DecisionTreeClassifier using BasicRandomSearch (SimBA).\n\nArguments\n\natk::BasicRandomSearch: Attack instance with epsilon and optional bounds.\nmodel::DecisionTreeClassifier: DecisionTree.jl classifier to attack.\nsample: NamedTuple with data and label fields.\n\nReturns\n\nAdversarial example (same shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"examples/#Tutorials-and-Examples-Overview","page":"Overview","title":"Tutorials & Examples Overview","text":"The Tutorials & Examples section provides end‑to‑end, runnable scripts that demonstrate how to use AdversarialAttacks.jl in realistic workflows. Each example focuses on a concrete combination of model type, dataset, and attack method, and comes with its own project environment for reproducible execution.\n\nThe current examples cover:\n\nA white‑box FGSM attack against a Flux CNN on MNIST.\nA black‑box Basic Random Search attack against a DecisionTree classifier on the Iris dataset.\n\nWhen working from the package environment in a Julia REPL started in the repository root, you can activate the dedicated examples environment and run the scripts as follows:\n\njulia> using Pkg\njulia> Pkg.activate(\"./examples\")   # activate the examples environment from the repository root\njulia> include(\"examples/whitebox_fgsm_flux_mnist.jl\")\njulia> include(\"examples/blackbox_basicrandomsearch_decisiontree_iris.jl\")\n\nFrom the examples/ directory, all scripts can be run with:\n\njulia --project=. whitebox_fgsm_flux_mnist.jl\njulia --project=. blackbox_basicrandomsearch_decisiontree_iris.jl\n\nEach tutorial logs training and attack statistics and opens Plots.jl visualizations to inspect the resulting adversarial examples.","category":"section"},{"location":"#AdversarialAttacks.jl","page":"Getting Started","title":"AdversarialAttacks.jl","text":"(Image: Dev) (Image: Build Status) (Image: Coverage) (Image: License)\n\nAdversarialAttacks.jl is a lightweight Julia package for experimenting with adversarial attacks against neural networks and tree‑based models, focusing on FGSM (white‑box) and random‑search–based (black‑box) attacks.\n\nCurrently, this package supports only models implemented as Flux chains (neural networks) and decision trees from the DecisionTree.jl package. Support for other model types may be added in the future.","category":"section"},{"location":"#Installation","page":"Getting Started","title":"Installation","text":"You can install the package via the Julia package manager. In the Julia REPL, run:\n\njulia> ]add https://github.com/shnaky/AdversarialAttacks.jl","category":"section"},{"location":"#Examples","page":"Getting Started","title":"Examples","text":"The following example shows how to create an adversarial sample from a single input sample using the FGSM attack:","category":"section"},{"location":"#FGSM-attack-Flux-Integration-(white-box)","page":"Getting Started","title":"FGSM attack - Flux Integration (white-box)","text":"julia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(\n           Dense(2, 2, tanh),\n           Dense(2, 2),\n           softmax,\n       )\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> sample = (data=rand(Float32, 2, 1), label=Flux.onehot(1, 1:2) )\n\njulia> adv_sample = attack(fgsm, model, sample)","category":"section"},{"location":"#BasicRandomSearch-attack-DecisionTree-integration-(black-box)","page":"Getting Started","title":"BasicRandomSearch attack - DecisionTree integration (black-box)","text":"julia> using AdversarialAttacks\n\njulia> using DecisionTree\n\njulia> classes = [1, 2, 3]\n\njulia> X = rand(24, 4) .* 4\n\njulia> y = vcat(\n    fill(classes[1], 8),\n    fill(classes[2], 8),\n    fill(classes[3], 8),\n)\n\njulia> tree = DecisionTreeClassifier(; classes = classes)\n\njulia> fit!(tree, X, y)\n\njulia> sample = (data = X[:, 1], label = y[1])\n\njulia> atk = BasicRandomSearch(epsilon = 0.1f0, max_iter = 50)\n\njulia> x_adv = attack(atk, tree, sample)","category":"section"},{"location":"#Batch-Example","page":"Getting Started","title":"Batch Example","text":"You can also apply an attack to a batch of samples represented as a tensor.\n\njulia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(Dense(4, 8, relu), Dense(8, 2), softmax)\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> X = rand(Float32, 4, 3)  # 3 samples, 4 features each\n\njulia> Y = Flux.onehotbatch([1, 2, 1], 1:2)  # labels for each sample\n\njulia> tensor = (data=X, label=Y)\n\njulia> adv_samples = attack(fgsm, model, tensor)","category":"section"},{"location":"#Evaluation-Example","page":"Getting Started","title":"Evaluation Example","text":"Get an evaluation report on your adversarial attack.\n\njulia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(Dense(4, 3), softmax)\n\njulia> test_data = [ (data=randn(Float32, 4), label=Flux.onehot(rand(1:3), 1:3)) for _ in 1:10 ]\n\njulia> fgsm = FGSM(epsilon=0.5)\n\njulia> report = evaluate_robustness(model, fgsm, test_data)\n\njulia> println(report)\n=== Robustness Evaluation Report ===\n\nDataset\n  Total samples evaluated        : 10\n  Clean-correct samples          : 4 / 10\n\nClean Performance\n  Clean accuracy                 : 40.0%\n\nAdversarial Performance\n  Adversarial accuracy           : 0.0%\n\nAttack Effectiveness\n  Successful attacks             : 4 / 4\n  Attack success rate (ASR)      : 100.0%\n  Robustness score (1 - ASR)     : 0.0%\n\nPerturbation Analysis (L_inf norm)\n  Maximum perturbation           : 0.5\n  Mean perturbation              : 0.5\n\nNotes\n  • Attack success is counted only when:\n    - the clean prediction is correct\n    - the adversarial prediction is incorrect\n===================================","category":"section"},{"location":"#License","page":"Getting Started","title":"License","text":"This package is licensed under the MIT License. See LICENSE for details.","category":"section"},{"location":"#AI-Assistance","page":"Getting Started","title":"AI Assistance","text":"This project uses AI coding assistants for routine maintenance tasks such as:\n\nCode style fixes and documentation improvements\nConsolidating imports and minor refactoring\nAddressing code review feedback\n\nAll AI-generated changes are reviewed before merging.","category":"section"},{"location":"evaluation/#Robustness-Evaluation-Suite","page":"Robustness Evaluation Suite","title":"Robustness Evaluation Suite","text":"This page documents Robustness Evaluation Suite.","category":"section"},{"location":"evaluation/#AdversarialAttacks.RobustnessReport","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.RobustnessReport","text":"RobustnessReport\n\nReport on model robustness against an adversarial attack. Printing a RobustnessReport (via println(report)) displays a nicely formatted summary including clean/adversarial accuracy, attack success rate, and robustness score.\n\nFields\n\nnum_samples::Int: Total samples evaluated\nnum_clean_correct::Int: Samples correctly classified before attack\nclean_accuracy::Float64: Accuracy on clean samples\nadv_accuracy::Float64: Accuracy on adversarial samples\nattack_success_rate::Float64: (ASR) Fraction of successful attacks (on correctly classified samples)\nrobustness_score::Float64: 1.0 - attacksuccessrate (ASR)\nnum_successful_attacks::Int: Number of successful attacks\nlinf_norm_max::Float64: Maximum L∞ norm of perturbations across all samples\nlinf_norm_mean::Float64: Mean L∞ norm of perturbations across all samples\nl2_norm_max::Float64: Maximum L2 norm of perturbations across all samples\nl2_norm_mean::Float64: Mean L2 norm of perturbations across all samples\nl1_norm_max::Float64: Maximum L1 norm of perturbations across all samples\nl1_norm_mean::Float64: Mean L1 norm of perturbations across all samples\n\nNote\n\nAn attack succeeds when the clean prediction is correct but the adversarial prediction is incorrect.\n\nThe L∞ norm measures the maximum absolute change in any feature of the input.\nThe L2 norm measures the Euclidean distance between original and adversarial samples.\nThe L1 norm measures the Manhattan distance (sum of absolute differences).\n\n\n\n\n\n","category":"type"},{"location":"evaluation/#AdversarialAttacks.calculate_metrics-NTuple{5, Any}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.calculate_metrics","text":"calculate_metrics(n_test, num_clean_correct, num_adv_correct,\n                  num_successful_attacks, l_norms)\n\nCompute accuracy, attack success, robustness, and perturbation norm statistics for adversarial evaluation.\n\nArguments\n\nn_test: Number of test samples.\nnum_clean_correct: Number of correctly classified clean samples.\nnum_adv_correct: Number of correctly classified adversarial samples.\nnum_successful_attacks: Number of successful adversarial attacks.\nl_norms: Dictionary containing perturbation norm arrays with keys :linf, :l2, and :l1.\n\nReturns\n\nA RobustnessReport containing accuracy, robustness, and norm summary metrics (maximum and mean) for all three norm types.\n\n\n\n\n\n","category":"method"},{"location":"evaluation/#AdversarialAttacks.compute_norm-Tuple{Any, Any, Real}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.compute_norm","text":"compute_norm(sample_data, adv_data, p::Real)\n\nCompute the Lp norm of the perturbation between original data and adversarial data.\n\nThis function uses LinearAlgebra.norm for optimal performance and numerical stability.\n\nArguments\n\nsample_data: Original sample data.\nadv_data: Adversarially perturbed version of sample_data.\np::Real: Order of the norm. Must be positive or Inf.\nCommon values: 1 (Manhattan/L1), 2 (Euclidean/L2), Inf (maximum/L∞).\n\nReturns\n\nFloat64: The Lp norm of the perturbation ||adv_data - sample_data||_p.\n\nExamples\n\noriginal = [1.0, 2.0, 3.0]\nadversarial = [1.5, 2.5, 3.5]\n\ncompute_norm(original, adversarial, 2)    # L2 (Euclidean) norm\ncompute_norm(original, adversarial, 1)    # L1 (Manhattan) norm\ncompute_norm(original, adversarial, Inf)  # L∞ (maximum) norm\n\nReferences\n\nLp space: https://en.wikipedia.org/wiki/Lp_space\n\n\n\n\n\n","category":"method"},{"location":"evaluation/#AdversarialAttacks.evaluate_robustness-Tuple{Any, Any, Any}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.evaluate_robustness","text":"evaluate_robustness(model, atk, test_data; num_samples=100)\n\nEvaluate model robustness by running attack on multiple samples.\n\nFor each sample, computes clean and adversarial predictions, tracks attack success, and calculates perturbation norms (L∞, L2, and L1).\n\nArguments\n\nmodel: The model to evaluate.\natk: The attack to use.\ntest_data: Collection of test samples.\nnum_samples::Int=100: Number of samples to test. If more than available samples, uses all available samples.\n\nReturns\n\nRobustnessReport: Report containing accuracy, attack success rate, robustness metrics,\n\nand perturbation statistics for L∞, L2, and L1 norms.\n\nExample\n\nreport = evaluate_robustness(model, FGSM(ε=0.1), test_data, num_samples=50)\nprintln(report)\n\n\n\n\n\n","category":"method"},{"location":"evaluation/#AdversarialAttacks.evaluation_curve-Tuple{Any, Type{<:AbstractAttack}, Vector{Float64}, Any}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.evaluation_curve","text":"evaluation_curve(model, atk_type, epsilons, test_data; num_samples=100)\n\nEvaluate model robustness across a range of attack strengths.\n\nFor each value in epsilons, an attack of type atk_type is instantiated and used to compute clean accuracy, adversarial accuracy, attack success rate, robustness score, and perturbation norms (L∞, L2, and L1).\n\nArguments\n\nmodel: Model to be evaluated.\natk_type: Adversarial attack type.\nepsilons: Vector of attack strengths.\ntest_data: Test dataset.\n\nKeyword Arguments\n\nnum_samples::Int=100: Number of samples used for each epsilon evaluation.\n\nReturns\n\nA dictionary containing evaluation metrics for each epsilon value:\n:epsilons: Attack strength values\n:clean_accuracy: Clean accuracy for each epsilon\n:adv_accuracy: Adversarial accuracy for each epsilon\n:attack_success_rate: Attack success rate for each epsilon\n:robustness_score: Robustness score (1 - ASR) for each epsilon\n:linf_norm_mean, :linf_norm_max: L∞ norm statistics\n:l2_norm_mean, :l2_norm_max: L2 norm statistics\n:l1_norm_mean, :l1_norm_max: L1 norm statistics\n\nExample\n\nresults = evaluation_curve(model, FGSM, [0.01, 0.05, 0.1], test_data, num_samples=100)\nprintln(\"Attack success rates: \", results[:attack_success_rate])\n\n\n\n\n\n","category":"method"}]
}
