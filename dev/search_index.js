var documenterSearchIndex = {"docs":
[{"location":"fgsm/#Fast-Gradient-Sign-Method-Attack","page":"Fast Gradient Sign Method Attack","title":"Fast Gradient Sign Method Attack","text":"This page documents the Fast Gradient Sign Method.","category":"section"},{"location":"fgsm/#FGSM-Implementation","page":"Fast Gradient Sign Method Attack","title":"FGSM Implementation","text":"","category":"section"},{"location":"fgsm/#AdversarialAttacks.FGSM","page":"Fast Gradient Sign Method Attack","title":"AdversarialAttacks.FGSM","text":"FGSM(; epsilon=0.1)\n\nA struct that can be used to create a white-box adversarial attack of type Fast Gradient Sign Method. Subtype of WhiteBoxAttack.\n\nArguments\n\nepsilon: Step size used to scale the sign of the gradient. Defaults to 0.1.\n\n\n\n\n\n","category":"type"},{"location":"fgsm/#AdversarialAttacks.craft-Tuple{Any, Flux.Chain, FGSM}","page":"Fast Gradient Sign Method Attack","title":"AdversarialAttacks.craft","text":"craft(sample, model, attack::FGSM)\n\nPerforms a Fast Gradient Sign Method (FGSM) white-box adversarial attack on the given model using the provided sample.\n\nArguments\n\nsample: Input sample as a named tuple with data and label.\nmodel::FluxModel: The machine learning (deep learning) model to be attacked.\nattack::FGSM: An instance of the FGSM.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"fgsm/#AdversarialAttacks.hyperparameters-Tuple{FGSM}","page":"Fast Gradient Sign Method Attack","title":"AdversarialAttacks.hyperparameters","text":"hyperparameters(atk::FGSM) -> Dict{String,Any}\n\nReturn hyperparameters for an FGSM attack.\n\nReturns\n\nDict{String,Any}: Dictionary containing attack hyperparameters (e.g., epsilon).\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#Attack-Interface","page":"Attack Interface","title":"Attack Interface","text":"This page documents the shared attack abstractions.","category":"section"},{"location":"attack_interface/#AdversarialAttacks.AbstractAttack","page":"Attack Interface","title":"AdversarialAttacks.AbstractAttack","text":"Abstract supertype for all adversarial attacks.\n\nExpected interface (to be implemented per concrete attack):\n\nname(::AbstractAttack)::String\nhyperparameters(::AbstractAttack)::Dict{String,Any}\ncraft(sample, model, atk::AbstractAttack; kwargs...) returning an adversarial example\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.WhiteBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.WhiteBoxAttack","text":"Abstract type for white-box adversarial attacks.\n\nWhite-box attacks have full access to the model's internals, including gradients, weights, and architecture. This enables the use of gradient-based optimization and other techniques to craft adversarial examples.\n\nUse this type when the attacker can inspect and manipulate the model's internal parameters and computations. If only input-output access is available, use BlackBoxAttack instead.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.BlackBoxAttack","page":"Attack Interface","title":"AdversarialAttacks.BlackBoxAttack","text":"Abstract type for black-box adversarial attacks.\n\nBlack-box attacks only have access to the model's input-output behavior, without knowledge of the model's internals, gradients, or architecture. These attacks typically rely on query-based methods (e.g., optimization via repeated queries) or transferability from surrogate models.\n\nUse BlackBoxAttack when you do not have access to the model's internal parameters or gradients, such as in deployed systems or APIs.  In contrast, use WhiteBoxAttack when you have full access to the model's internals and can leverage gradient information for crafting adversarial examples.\n\n\n\n\n\n","category":"type"},{"location":"attack_interface/#AdversarialAttacks.name-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.name","text":"name(atk::AbstractAttack) -> String\n\nHuman-readable name for an attack.\n\nReturns\n\nString: String representation of the attack type.\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#AdversarialAttacks.hyperparameters-Tuple{AbstractAttack}","page":"Attack Interface","title":"AdversarialAttacks.hyperparameters","text":"hyperparameters(atk::AbstractAttack) -> Dict{String,Any}\n\nReturn hyperparameters for an attack.\n\nReturns\n\nDict{String,Any}: Dictionary of attack hyperparameters.\n\n\n\n\n\n","category":"method"},{"location":"attack_interface/#AdversarialAttacks.craft","page":"Attack Interface","title":"AdversarialAttacks.craft","text":"craft(sample, model, attack::AbstractAttack; kwargs...) -> adversarial_sample\n\nCraft an adversarial example by applying the attack to a sample.\n\nArguments\n\nsample: Input sample to perturb (e.g., image, text)\nmodel: Target model to attack\nattack::AbstractAttack: Attack configuration and algorithm\nkwargs...: Additional attack-specific parameters\n\nReturns\n\nAdversarial example with the same shape as the input sample\n\n\n\n\n\ncraft(sample, model::Flux.Chain, attack::BasicRandomSearch)\n\nPerforms a black-box adversarial attack on the given model using the provided sample using the Basic Random Search variant SimBA.\n\nArguments\n\nsample: The input sample to be changed.\nmodel::Flux.Chain: The machine learning (deep learning, classical machine learning) model to be attacked.\nattack::BasicRandomSearch: An instance of the BasicRandomSearch (BlackBox) attack.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\ncraft(sample, model::DecisionTreeClassifier, attack::BasicRandomSearch)\n\nPerforms a black-box adversarial attack on a DecisionTreeClassifier using BasicRandomSearch (SimBA).\n\nArguments\n\nsample: NamedTuple with data and label fields.\nmodel::DecisionTreeClassifier: DecisionTree.jl classifier to attack.\nattack::BasicRandomSearch: Attack instance with epsilon and optional bounds.\n\nReturns\n\nAdversarial example (same shape as sample.data).\n\n\n\n\n\ncraft(sample, model, attack::FGSM)\n\nPerforms a Fast Gradient Sign Method (FGSM) white-box adversarial attack on the given model using the provided sample.\n\nArguments\n\nsample: Input sample as a named tuple with data and label.\nmodel::FluxModel: The machine learning (deep learning) model to be attacked.\nattack::FGSM: An instance of the FGSM.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"function"},{"location":"interface/#Interface","page":"Interface","title":"Interface","text":"This page documents the Interface for user interaction.","category":"section"},{"location":"interface/#AdversarialAttacks.attack","page":"Interface","title":"AdversarialAttacks.attack","text":"attack(atk, model, sample; kwargs...)\n\nApply an adversarial attack to a sample using the given model.\n\nArguments\n\natk::AbstractAttack: The attack object to apply.\nmodel: The model to attack. Supports:\nFlux.Chain (for white-box and black-box attacks)\nDecisionTreeClassifier (for black-box attacks)\nsample::AbstractArray{<:Number} or NamedTuple:\nRaw input array, or\nNamedTuple with data and label fields.\nkwargs...: Additional keyword arguments.\n\nReturns\n\nAdversarial sample produced by the attack.\n\nNotes\n\nWhiteBoxAttack is supported for Flux.Chain.\nBlackBoxAttack is supported for both Flux.Chain and DecisionTreeClassifier (treated as black-box models, using only model outputs).\n\n\n\n\n\n","category":"function"},{"location":"interface/#AdversarialAttacks.benchmark","page":"Interface","title":"AdversarialAttacks.benchmark","text":"benchmark(atk::AbstractAttack, model, dataset, metric::Function; kwargs...)\n\nEvaluate attack performance on a dataset with labels using a given metric.\n\nArguments\n\natk::AbstractAttack: Attack algorithm\nmodel: Target model to attack\ndataset: Dataset with samples and labels\nmetric::Function: Evaluation metric with signature metric(model, adv_samples, labels)\n\nReturns\n\nScalar metric value representing attack performance\n\n\n\n\n\n","category":"function"},{"location":"blackbox_subtypes/#Black-Box-Algorithms","page":"Black Box Attacks","title":"Black Box Algorithms","text":"This page documents the black box algorithms.","category":"section"},{"location":"blackbox_subtypes/#AdversarialAttacks.BasicRandomSearch","page":"Black Box Attacks","title":"AdversarialAttacks.BasicRandomSearch","text":"BasicRandomSearch(; epsilon=0.1, bounds=nothing)\n\nSubtype of BlackBoxAttack. Creates adversarial examples using the SimBA random search algorithm.\n\nArguments\n\nepsilon: Step size for perturbations (default: 0.1)\nbounds: Optional vector of (lower, upper) tuples specifying per-feature bounds.           If nothing, defaults to [0, 1] for all features (suitable for normalized images).           For tabular data, provide bounds matching feature ranges, e.g.,           [(4.3, 7.9), (2.0, 4.4), ...] for Iris-like data.\n\n\n\n\n\n","category":"type"},{"location":"blackbox_subtypes/#AdversarialAttacks.craft-Tuple{Any, Flux.Chain, BasicRandomSearch}","page":"Black Box Attacks","title":"AdversarialAttacks.craft","text":"craft(sample, model::Flux.Chain, attack::BasicRandomSearch)\n\nPerforms a black-box adversarial attack on the given model using the provided sample using the Basic Random Search variant SimBA.\n\nArguments\n\nsample: The input sample to be changed.\nmodel::Flux.Chain: The machine learning (deep learning, classical machine learning) model to be attacked.\nattack::BasicRandomSearch: An instance of the BasicRandomSearch (BlackBox) attack.\n\nReturns\n\nAdversarial example (same type and shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"blackbox_subtypes/#AdversarialAttacks.craft-Tuple{Any, DecisionTree.DecisionTreeClassifier, BasicRandomSearch}","page":"Black Box Attacks","title":"AdversarialAttacks.craft","text":"craft(sample, model::DecisionTreeClassifier, attack::BasicRandomSearch)\n\nPerforms a black-box adversarial attack on a DecisionTreeClassifier using BasicRandomSearch (SimBA).\n\nArguments\n\nsample: NamedTuple with data and label fields.\nmodel::DecisionTreeClassifier: DecisionTree.jl classifier to attack.\nattack::BasicRandomSearch: Attack instance with epsilon and optional bounds.\n\nReturns\n\nAdversarial example (same shape as sample.data).\n\n\n\n\n\n","category":"method"},{"location":"#AdversarialAttacks.jl","page":"Getting Started","title":"AdversarialAttacks.jl","text":"(Image: Dev) (Image: Build Status) (Image: Coverage) (Image: License)\n\nAdversarialAttacks.jl is a lightweight Julia package for experimenting with adversarial attacks against neural networks and tree‑based models, focusing on FGSM (white‑box) and random‑search–based (black‑box) attacks.\n\nCurrently, this package supports only models implemented as Flux chains (neural networks) and decision trees from the DecisionTree.jl package. Support for other model types may be added in the future.","category":"section"},{"location":"#Installation","page":"Getting Started","title":"Installation","text":"You can install the package via the Julia package manager. In the Julia REPL, run:\n\njulia> ]add https://github.com/shnaky/AdversarialAttacks.jl","category":"section"},{"location":"#Examples","page":"Getting Started","title":"Examples","text":"The following example shows how to create an adversarial sample from a single input sample using the FGSM attack:","category":"section"},{"location":"#FGSM-attack-Flux-Integration-(white-box)","page":"Getting Started","title":"FGSM attack - Flux Integration (white-box)","text":"julia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(\n           Dense(2, 2, tanh),\n           Dense(2, 2),\n           softmax,\n       )\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> sample = (data=rand(Float32, 2, 1), label=Flux.onehot(1, 1:2) )\n\njulia> adv_sample = attack(fgsm, model, sample)","category":"section"},{"location":"#BasicRandomSearch-attack-DecisionTree-integration-(black-box)","page":"Getting Started","title":"BasicRandomSearch attack - DecisionTree integration (black-box)","text":"julia> using AdversarialAttacks\n\njulia> using DecisionTree\n\njulia> classes = [1, 2, 3]\n\njulia> X = rand(24, 4) .* 4\n\njulia> y = vcat(\n    fill(classes[1], 8),\n    fill(classes[2], 8),\n    fill(classes[3], 8),\n)\n\njulia> tree = DecisionTreeClassifier(; classes = classes)\n\njulia> fit!(tree, X, y)\n\njulia> sample = (data = X[:, 1], label = y[1])\n\njulia> atk = BasicRandomSearch(epsilon = 0.1f0)\n\njulia> x_adv = attack(atk, tree, sample)","category":"section"},{"location":"#Batch-Example","page":"Getting Started","title":"Batch Example","text":"You can also apply an attack to a batch of samples represented as a tensor.\n\njulia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(Dense(4, 8, relu), Dense(8, 2), softmax)\n\njulia> fgsm = FGSM(; epsilon=0.3)\n\njulia> X = rand(Float32, 4, 3)  # 3 samples, 4 features each\n\njulia> Y = Flux.onehotbatch([1, 2, 1], 1:2)  # labels for each sample\n\njulia> tensor = (data=X, label=Y)\n\njulia> adv_samples = attack(fgsm, model, tensor)","category":"section"},{"location":"#Evaluation-Example","page":"Getting Started","title":"Evaluation Example","text":"Get an evaluation report on your adversarial attack.\n\njulia> using AdversarialAttacks\n\njulia> using Flux\n\njulia> model = Chain(Dense(4, 3), softmax)\n\njulia> test_data = [ (data=randn(Float32, 4), label=Flux.onehot(rand(1:3), 1:3)) for _ in 1:10 ]\n\njulia> fgsm = FGSM(epsilon=0.5)\n\njulia> report = evaluate_robustness(model, fgsm, test_data)\n\njulia> println(report)\n=== Robustness Evaluation Report ===\n\nDataset\n  Total samples evaluated        : 10\n  Clean-correct samples          : 4 / 10\n\nClean Performance\n  Clean accuracy                 : 40.0%\n\nAdversarial Performance\n  Adversarial accuracy           : 30.0%\n\nAttack Effectiveness\n  Successful attacks             : 1 / 4\n  Attack success rate (ASR)      : 25.0%\n  Robustness score (1 - ASR)     : 75.0%\n\nNotes\n  • Attack success is counted only when:\n    - the clean prediction is correct\n    - the adversarial prediction is incorrect\n===================================","category":"section"},{"location":"#License","page":"Getting Started","title":"License","text":"This package is licensed under the MIT License. See LICENSE for details.","category":"section"},{"location":"evaluation/#Robustness-Evaluation-Suite","page":"Robustness Evaluation Suite","title":"Robustness Evaluation Suite","text":"This page documents Robustness Evaluation Suite.","category":"section"},{"location":"evaluation/#AdversarialAttacks.RobustnessReport","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.RobustnessReport","text":"RobustnessReport\n\nReport on model robustness against an adversarial attack. Printing a RobustnessReport (via println(report)) displays a nicely formatted summary including clean/adversarial accuracy, attack success rate, and robustness score.\n\nFields\n\nnum_samples::Int: Total samples evaluated\nnum_clean_correct::Int: Samples correctly classified before attack\nclean_accuracy::Float64: Accuracy on clean samples\nadv_accuracy::Float64: Accuracy on adversarial samples\nattack_success_rate::Float64: (ASR) Fraction of successful attacks (on correctly classified samples)\nrobustness_score::Float64: 1.0 - attacksuccessrate (ASR)\nnum_successful_attacks::Int: Number of successful attacks\n\nNote\n\nAn attack succeeds when the clean prediction is correct but the adversarial prediction is incorrect.\n\n\n\n\n\n","category":"type"},{"location":"evaluation/#AdversarialAttacks.evaluate_robustness-Tuple{Any, Any, Any}","page":"Robustness Evaluation Suite","title":"AdversarialAttacks.evaluate_robustness","text":"evaluate_robustness(model, attack, test_data; num_samples=100)\n\nEvaluate model robustness by running attack on multiple samples.\n\nArguments\n\nmodel: The model to evaluate.\nattack: The attack to use.\ntest_data: Collection of test samples.\nnum_samples::Int=100: Number of samples to test. If more than available samples, uses all available samples.\n\nReturns\n\nRobustnessReport: Report containing accuracy, attack success rate, and robustness metrics\n\nExample\n\nreport = evaluate_robustness(model, FGSM(ε=0.1), test_data, num_samples=50)\nprintln(report)\n\n\n\n\n\n","category":"method"}]
}
